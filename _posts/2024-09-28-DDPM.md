---
title:  "DDPM"
excerpt: "stable diffusion"

categories:
  - Paper
tags:
  - AI
  - Computer Vision
  - Paper
last_modified_at: 2024-09-28T08:06:00-05:00
---

# Stable Diffusion 

<p align="center"><img src="/assets/images/cat_1.png" height="25%" width="25%">　　<img src="/assets/images/cat_8.png" height="25%" width="25%">　　<img src="/assets/images/cat_2.png" height="25%" width="25%"></p>

## VAE

### Maximum Likelihood
**VAE(Variational Autoencoders)** 는 Generative model로 Autoencoders와는 반대로 Decoder부분을 학습시키기 위해 만들어졌다. 
MLE(Maximum Likelihood Estimation)관점에서의 모델의 학습에 대해 먼저 설명하면 input $z$와 target $x$가 있을 때, 
$f_{\theta}(\cdot)$ 는 모델(가우시안, 베르누이.. )이고, 최종 목표는 target이 나올 확률인 $p(x | f_{\theta}(z))$가 최대가 되도록 하는
$\theta$를 찾는 것이다. MLE에서는 학습전에 학습할 확률분포(가우시안, 베르누이.. )를 먼저 정하고, 모델의 출력은 이 확률 분포를 정하기
위한 파라미터(가우시안의 경우 $\mu, \sigma^2$)라고 해석할 수 있다. 결과적으로 target을 잘 생성하는 모델 파라미터
$\theta$는 $\theta^* = \underset{\theta}{\arg\min} [-\log(p(x \vert f_{\theta}(z)))]$가 된다. 
이렇게 찾은 $\theta^*$는 확률분포를 찾은 것이므로 결과에 대한 sampling이 가능하고, 이 sampling에 따라 다양한 이미지가 생성될 수 있는 것이다.

VAE의 Decoder도 위와 비슷하다. Encoder를 통해 sampling된 데이터 $z$ (Latent Variable)가 있고 Generator $g_{\theta}(\cdot)$와 Target $x$가 있을 때, training data에 있는 $x$가 나올 확률을 구하는 것을 목적으로 한다. 이때 $z$는 controller로서 생성될 이미지를 조정하는 역할을 할 수 있다. 예를 들면 고양이의 귀여움을 조정하여 더 귀여운 고양이 이미지를 생성하는 것이다.

다시 돌아와서 결과적으로 VAE의 목적은 모든 training data $x$에 대해 $x$가 나올 확률 $p(x)$를 구하는 것이 목적이다. 이때 training data에 있는 sample과 유사한 sample을 생성하기 위해서 prior 값을 이용하는데, 이 값이 Latent Variable인 $z$가 나올 확률 $p(z)$이고, $p(x)$ 는 $\int p(x \vert g_{\theta}(z))p(z)dz=p(x)$로 구해진다. **MLE(Maximum Likelihood Estimation)** 와 **MAP(Maximum A Posteriori)** 에 대한 자세한 내용은 reference에 있다. 

### Prior Distribution

<p align="center"><img width="746" alt="prior-distribution" src="/assets/images/prior-distribution.png"></p>

앞서 말했듯이 $z$는 controller 역할을 하기 때문에 $z$를 잘 조정할 수 있어야 한다. 이때 $z$는 고차원 input에 대한 manifold 상에서의 값들인데, generator의 input으로 들어가기 위해 sampling된 값이 이 manifold 공간을 잘 대표하는가? 라는 질문이 나온다. 위 사진을 보면 왼쪽에 normally-distributed 된 분포가 있을 때 해당 분포에 

$$g(z) = \frac{z}{10} + \frac{z}{ \parallel z \parallel}$$ 

를 적용하면 오른쪽 처럼 ring 형태의 분포가 나오는 것을 확인할 수 있다. 이처럼 간단한 변형으로 manifold를 대표할 수 있기 때문에 모델이 DNN 이라면, 학습해야 하는 manifold가 복잡하다 하더라도, DNN의 한 두개의 layer가 manifold를 찾기위한 역할로 사용될 수 있다. 따라서 Prior Distribution을 normal distribution과 같은 간단한 distribution으로 해도 상관없다.  


### Variational Inference
$p(x \vert g_{\theta}(z))$의 likelihood가 최대가 되는 것이 목표라면 Maximum Likelihood Estimation를 직접적으로 사용해서 구하면 될거 같은데 실제론 그렇지 않다. 그 이유는 가우시안 분포라 가정했을 때, $p(x \vert g_{\theta}(z))$의 log loss인 $-\log(p(x \vert g_{\theta}(z)))$는 Mean Squared Error와 같아진다. 즉, MSE의 관점에서 loss가 작은 것이 $p(x)$에 더 크게 관여하는데, MSE loss가 작은 이미지가 실제 의미적으로 더 가까운 이미지가 아닌 경우가 많기 때문에 올바른 방향으로 학습할 수가 없다. 

$${\parallel x - z_{bad} \parallel}^2 < {\parallel x - z_{good} \parallel}^2 \to p(x \vert g_{\theta}(z_{bad})) > p(x \vert g_{\theta}(z_{good}))$$

예를 들면 원래 고양이 이미지에서 일부분이 조금 잘린 이미지 $a$와 한 pixel씩 옆으로 이동한 이미지 $b$가 있다고 하면 $b$는 pixel만 옆으로 밀렸을 뿐 고양이 그대로 이지만 $a$는 이미지가 잘렸기 때문에 의미적으론 $b$가 $a$보다 고양이에 가까운데, MSE 관점에서는 $b$의 loss가 더 크게 나오게 된다. 

이러한 문제를 해결하기 위해 Variational Inference가 나오게 된다. 기존 prior에서 sampling을 하니 학습이 잘 안되니까 $z$ 를 prior에서 sampling하지 말고 target인 $x$ 와 유사한 sample이 나올 수 있는 이상적인 확률분포 $p(z \vert x)$ 로 부터 sampling한다. 이때 우리는 
$p(z \vert x)$ 가 무엇인지 알지 못하기 때문에, 이미 알고 있는 확률 분포(가우시안..) $q_{\phi}(z \vert x)$ 를 임의로 택하고 그것의 파라미터 $\phi$ 를 조정하여 $p(z \vert x)$ 와 유사하게 되도록 하는 것이다. 그렇게 이상적인 확률분포에 근사된 $q_{\phi}$ 를 통해서 $z$ 를 sampling하게 된다. 

$$p(z \vert x) \approx q_{\phi}(z \vert x) \sim z$$


### ELBO
지금까지의 내용을 정리하면 우리가 구하고자 하는 것은 $p(x)$였고, 이를 위해 Prior Distribution을 사용했으며, 그냥 prior에서 sampling하려니 잘 학습이 안되서 이상적인 확률분포 $p(z|x)$ 를 근사한 $q_{\phi}$를 사용하게 됐다. 이 4개간의 관계식에서 loss를 유도하는 과정에 우리가 찾아야 하는 ELBO(Evidence LowerBOund)가 나오게 된다. 

우선 $\log(p(x))$에서 시작해서 ELBO를 유도하는 과정을 정리하면 아래와 같다. 
 
$$\begin{aligned}
\log(p(x)) &= \int \log(p(x))q_{\phi}(z|x)dz 　 \leftarrow \int q_{\phi}(z|x)dz = 1 \\ 
&=\int \log\left(\frac{p(x, z)}{p(z|x)}\right)q_{\phi}(z|x)dz 　 \leftarrow p(x) = \frac{p(x, z)}{p(z|x)} \\
&=\int \log\left(\frac{p(x, z)}{q_{\phi}(z|x)}\cdot\frac{q_{\phi}(z|x)}{p(z|x)}\right)q_{\phi}(z|x)dz \\ 
&=\int \underbrace{\log\left(\frac{p(x, z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz}_ {ELBO(\phi)} + \int \underbrace{\log\left(\frac{q_{\phi}(z|x)}{p(z|x)}\right)q_{\phi}(z|x)dz}_ {KL\left(q_{\phi}(z|x) \ || \ p(z|x)\right)} \\ 
\end{aligned}$$

여기서 $KL \left(q_{\phi}(z \vert x) \ \parallel \ p(z \vert x)\right)$ term은 Kullback–Leibler divergence로 두 확률분포 간의 거리($\ge 0$)를 구한다.
우리가 원하는 건 $q_{\phi}(z \vert x)$가 $p(z \vert x)$에 최대한 가까워 져야 하므로 $KL$을 최소화 하는 $q_{\phi}(z \vert x)$의 $\phi$를 찾아야 하는데 $p(z \vert x)$를 모르기 때문에 KL을 최소화 하는 대신 $ELBO$를 최대화 하는 $\phi$를 찾으면 된다. 

$$\log(p(x)) = ELBO(\phi) + KL(\left(q_{\phi}(z|x) \ || \ p(z|x)\right)$$ 

$$q_{\phi^*}(z|x) = \underset{\phi}{\arg\max} \ ELBO(\phi)$$

$ELBO$를 최대화 하기 위해 $ELBO$ term을 다시 전개하면 다음과 같다. 

$$\begin{aligned}
ELBO(\phi) &= \int \log \left(\frac{p(x, z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz \\
&= \int \log \left(\frac{p(x|z)p(z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz \\  
&= \int \log \left(p(x|z)\right)q_{\phi}(z|x)dz - \int \log \left(\frac{q_{\phi}(z|x)}{p(z)}\right)q_{\phi}(z|x)dz \\ 
&= \mathbb{E}_ {q_{\phi}(z|x)} \left[\log\left(p(x|z)\right)\right] - KL\left(q_{\phi}(z|x) \ || \ p(z)\right)
\end{aligned}$$

그래서 정리하면 $ELBO$를 최대화 하는데 있어서 $\phi$, $\theta$로 총 2개에 대한 Optimization Problem을 해결해야 하고 이는 아래와 같다. 

**Optimization Problem 1 on $\phi$: Variational Inference**

$$\log(p(x)) \ge \mathbb{E}_ {q_{\phi}(z \vert x)} \left[\log\left(p(x \vert z)\right)\right] - KL\left(q_{\phi}(z \vert x) \ \parallel \ p(z)\right) = ELBO(\phi)$$

첫 번째로 $ELBO$를 maximize하는 $\phi$를 찾는데, 이 과정이 이상적인 sampling function을 찾는 것이다.    

**Optimization Problem 2 on $\theta$: Maximum likelihood**

$$-\sum_i \log(p(x_i)) \le -\sum_i \lbrace\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] - KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right)\rbrace$$

두 번째는 이상적인 sampling function을 찾았으므로 여기서 $z$를 sampling해서 $z$로 부터 target $x$가 나오게 하는 Conditional probability $p(x \vert g_{\theta}(z))$가 최대가 되도록 하는 확률분포를 찾는 것이다.        
$\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left(p(x_i \vert g_{\theta}(z))\right)\right]$는 $q_{\phi}$에서 sampling한 $z$에 대한 $\log\left(p(x_i \vert g_{\theta}(z))\right)$를 의미한다.  

**Final Optimization Problem**

$$\underset{\phi, \theta}{\arg\min} \sum_i -\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] + KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right)$$

결국 위 두 Optimization Problem을 종합하는 식이 위와 같고, 이 식이 $ELBO$를 최대화하는 것과 같게 된다. 

정리해서 이상적으로 sampling을 해주는 $q_{\phi}(z \vert x)$를 Encoder, Posterior, Inference Network 등으로 부르고, sampling된 $z$로 부터 이미지를 generate 해주는 $g_{\theta}(x \vert z)$를 Decoder, Generator, Generation Network 등으로 부른다. 

### Loss Function 
결과적으로 ELBO를 최대화 하기 위한 Loss는 아래와 같이 표현된다. 

$$L_i(\phi, \theta, x_i) = \underbrace{-\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right]}_ \text{Reconstruction  Error} + \underbrace{KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right)}_ \text{Regularization}$$ 


Reconstruction  Error term은 앞서 말했던 MSE(가우시안의 경우)로 계산되는 부분이다. 해당 term을 결론적으로 보면 $x$를 넣었을 때 $x$가 나올 확률에 대한 것이기 때문에 Reconstruction  Error라고 한다. 만약 가정을 베르누이 분포라고 하면 MSE가 아닌 Cross Entropy가 된다. 

Regularization는 같은 Reconstruction Error를 갖는 $q_{\phi}$가 여럿 있다면, 그 중에서도 prior $p(z)$와 가까운 $q_{\phi}$를 고르라는 것으로, 생성 데이터에 대한 통제 조건을 prior에 부여하고, 이와 유사해야 한다는 조건을 부여한 것이다. 

그럼 이제 $ELBO$를 실제 어떻게 계산하는지 알아보기 전에 $q_{\phi}$를 gaussian distribution $q_{\phi} \sim N(\mu_i, \sigma_i^2I)$, $p(z)$를 normal distribution $p(z) \sim N(0, 1)$으로 가정한다고 하자. 

우선 Regularization term의 경우 2개의 가우시안 분포 간의 KL divergence는 아래와 같이 계산된다고 수학적으로 알려져있다.

$$D_{KL}(\mathcal{N}_0 \ \parallel \ \mathcal{N}_1) = \frac{1}{2}\left[ tr\left({Σ_1}^{-1}Σ_0\right) + (\mu_1 - \mu_0)^T {Σ_1}^{-1}(\mu_1 - \mu_0) - k + \ln\frac{\vert Σ_1 \vert}{\vert Σ_0 \vert}\right]$$

$$tr(A) = \sum_{i} A_{ii}$$

이는 KL divergence $D_{KL}(p \parallel q) = \int_x p(x)\log \frac{p(x)}{q(x)}$ 를 구할 때 다변량 정규 분포의 pdf가 아래와 같음을 이용하여 유도된 것으로 자세한 내용은 reference에 있다. 

$$p(x) = \frac{1}{(2\pi)^{k/2} \vert Σ \vert^{1/2}} \exp \left(- \frac{1}{2} (x-\mu)^{T} {Σ}^{-1} (x - \mu) \right) \\  　　
Σ = \mathbb{E} \left[ (x - \mu) (x - \mu)^T \right] = 
\begin{vmatrix}
\sigma_{11}^2 & \sigma_{12}^2 & \cdots & \sigma_{1p}^2 \\ 
\sigma_{21}^2 & \sigma_{22}^2 & \cdots & \sigma_{2p}^2 \\ 
\vdots & \vdots & \ddots & \vdots \\  
\sigma_{p1}^2 & \sigma_{p2}^2 & \cdots & \sigma_{pp}^2
\end{vmatrix}$$

여기서 $Σ$ 은 공분산 행렬이고, $(x-\mu)^{T} {Σ}^{-1} (x - \mu)$는 마할라노비스 거리(Mahalanobis distance)로 $x$가 평균값 $\mu$에서 얼마나 많은 표준 편차만큼 떨어져 있는가를 나타낸다. 

이에 따라 앞서 가정한 $q_{\phi} \sim N(\mu_i, \sigma_i^2I)$, $p(z) \sim N(0, 1)$ 대로 KL term을 계산하면 아래와 같이 된다.

$$\begin{aligned}
KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right) &= \frac{1}{2} \left\lbrace tr(\sigma_i^2I) + \mu_i^T\mu_i - J + \ln \frac{1}{\prod \sigma_{i,j}^2} \right\rbrace \\ 
&= \frac{1}{2} \left\lbrace \sum_{j=1}^J \sigma_{i,j}^2 + \sum_{j=1}^J \mu_{i,j}^2 - J - \sum_{j=1}^J \ln(\sigma_{i,j}^2) \right\rbrace \\ 
&= \frac{1}{2}\sum_{j=1}^J \left(\mu_{i,j}^2 + \sigma_{i,j}^2 - \ln(\sigma_{i,j}^2) - 1\right)
\end{aligned}$$

　　　　　 　　　　 　　　 $J$ : $\text{dimension}$  
　　　　　 　　　　　 　　 $\mu, \sigma$ : $q_{\phi} \sim N(\mu_i, \sigma_i^2I)$

Reconstruction  Error term의 경우 원래라면 아래처럼 기댓값을 구할 때 적분을 해야하지만, 대신 Monte Carlo method로 $L$개를 sampling하여 구한다. 

$$\begin{aligned}
\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] &= \int \log(p_{\theta}(x_i \vert z))q_{\phi}(z \vert x_i)dz \\ 
&\approx \frac{1}{L}\sum_{z^{i,l}} \log\left(p_{\theta}(x_i \vert z^{i,l})\right)
\end{aligned}$$

이때 문제는 $q_{\phi}$에서 random으로 sampling을 하기 때문에 backpropagation에서의 편미분이 불가능하다는 것이다. 그래서 이를 해결하기 위해 reparameterization trick을 사용하는데, 이는 normal distribution $\mathcal{N}(0,1)$에서 sampling한 $\epsilon$에 대해 아래처럼 표현될 수 있다는 점을 이용하여 backpropagation이 가능하도록 한 것이다. 

$$z^{i,l} \sim \mathcal{N}(\mu_i, \sigma_i^2I) 　\to　 z^{i,l} = \mu_i + \sigma_i^2 \odot \epsilon 　　\epsilon \sim \mathcal{N}(0,1)$$

Reconstruction Error를 정리하면 아래와 같고 Monte Carlo에서의 sampling의 경우 $L=1$로 하나만 sampling하는 경우가 많다. 

$$\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] = \int \log(p_{\theta}(x_i \vert z))q_{\phi}(z \vert x_i)dz \approx \frac{1}{L}\sum_{z^{i,l}} \log\left(p_{\theta}(x_i \vert z^{i,l})\right) \approx \log\left(p_{\theta}(x_i \vert z^i)\right) 　\leftarrow L=1$$

그러면 $\log\left(p_{\theta}(x_i \vert z^i)\right)$의 값만 구하면 되는데, 이미지처리의 경우 $p_{\theta}$를 gaussian대신 bernoulli 분포로 정하고 계산한다. 따라서 bernoulli distribution에 따라 구하면 아래와 같이 Cross Entropy의 형태가 나온다. 

$$\begin{aligned}
\log\left(p_{\theta}(x_i \vert z^i)\right) &= \log \prod_{j=1}^D p_{\theta}(x_{i,j} \vert z^i) = \sum_{j=1}^D \log p_{\theta}(x_{i,j} \vert z^i)　\leftarrow i.i.d \\ 
&= \sum_{j=1}^D \log p_{i,j}^{x_{i,j}}(1-p_{i,j})^{1-x_{i,j}}　\leftarrow p_{i,j} \circeq network \ output \\ 
&= \sum_{j=1}^D x_{i,j} \log p_{i,j} + (1-x_{i,j}) \log (1-p_{i,j})　\leftarrow Cross \ entropy
\end{aligned}$$
