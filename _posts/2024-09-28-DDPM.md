---
title:  "DDPM"
excerpt: "stable diffusion"

categories:
  - Paper
tags:
  - AI
  - Computer Vision
  - Paper
last_modified_at: 2024-09-28T08:06:00-05:00
---

# Stable Diffusion 

<p align="center"><img src="/assets/images/cat_1.png" height="25%" width="25%">　　<img src="/assets/images/cat_8.png" height="25%" width="25%">　　<img src="/assets/images/cat_2.png" height="25%" width="25%"></p>

## VAE

### Maximum Likelihood
**VAE(Variational Autoencoders)** 는 Generative model로 Autoencoders와는 반대로 Decoder부분을 학습시키기 위해 만들어졌다. 
MLE(Maximum Likelihood Estimation)관점에서의 모델의 학습에 대해 먼저 설명하면 input $z$와 target $x$가 있을 때, 
$f_{\theta}(\cdot)$ 는 모델(가우시안, 베르누이.. )이고, 최종 목표는 target이 나올 확률인 $p(x | f_{\theta}(z))$가 최대가 되도록 하는
$\theta$를 찾는 것이다. MLE에서는 학습전에 학습할 확률분포(가우시안, 베르누이.. )를 먼저 정하고, 모델의 출력은 이 확률 분포를 정하기
위한 파라미터(가우시안의 경우 $\mu, \sigma^2$)라고 해석할 수 있다. 결과적으로 target을 잘 생성하는 모델 파라미터
$\theta$는 $\theta^* = \underset{\theta}{\arg\min} [-\log(p(x \vert f_{\theta}(z)))]$가 된다. 
이렇게 찾은 $\theta^*$는 확률분포를 찾은 것이므로 결과에 대한 sampling이 가능하고, 이 sampling에 따라 다양한 이미지가 생성될 수 있는 것이다.

VAE의 Decoder도 위와 비슷하다. Encoder를 통해 sampling된 데이터 $z$ (Latent Variable)가 있고 Generator $g_{\theta}(\cdot)$와 Target $x$가 있을 때, training data에 있는 $x$가 나올 확률을 구하는 것을 목적으로 한다. 이때 $z$는 controller로서 생성될 이미지를 조정하는 역할을 할 수 있다. 예를 들면 고양이의 귀여움을 조정하여 더 귀여운 고양이 이미지를 생성하는 것이다.

다시 돌아와서 결과적으로 VAE의 목적은 모든 training data $x$에 대해 $x$가 나올 확률 $p(x)$를 구하는 것이 목적이다. 이때 training data에 있는 sample과 유사한 sample을 생성하기 위해서 prior 값을 이용하는데, 이 값이 Latent Variable인 $z$가 나올 확률 $p(z)$이고, $p(x)$ 는 $\int p(x \vert g_{\theta}(z))p(z)dz=p(x)$로 구해진다. **MLE(Maximum Likelihood Estimation)** 와 **MAP(Maximum A Posteriori)** 에 대한 자세한 내용은 reference에 있다. 

### Prior Distribution

<p align="center"><img width="746" alt="prior-distribution" src="/assets/images/prior-distribution.png"></p>

앞서 말했듯이 $z$는 controller 역할을 하기 때문에 $z$를 잘 조정할 수 있어야 한다. 이때 $z$는 고차원 input에 대한 manifold 상에서의 값들인데, generator의 input으로 들어가기 위해 sampling된 값이 이 manifold 공간을 잘 대표하는가? 라는 질문이 나온다. 위 사진을 보면 왼쪽에 normally-distributed 된 분포가 있을 때 해당 분포에 

$$g(z) = \frac{z}{10} + \frac{z}{ \parallel z \parallel}$$ 

를 적용하면 오른쪽 처럼 ring 형태의 분포가 나오는 것을 확인할 수 있다. 이처럼 간단한 변형으로 manifold를 대표할 수 있기 때문에 모델이 DNN 이라면, 학습해야 하는 manifold가 복잡하다 하더라도, DNN의 한 두개의 layer가 manifold를 찾기위한 역할로 사용될 수 있다. 따라서 Prior Distribution을 normal distribution과 같은 간단한 distribution으로 해도 상관없다.  


### Variational Inference
$p(x \vert g_{\theta}(z))$의 likelihood가 최대가 되는 것이 목표라면 Maximum Likelihood Estimation를 직접적으로 사용해서 구하면 될거 같은데 실제론 그렇지 않다. 그 이유는 가우시안 분포라 가정했을 때, $p(x \vert g_{\theta}(z))$의 log loss인 $-\log(p(x \vert g_{\theta}(z)))$는 Mean Squared Error와 같아진다. 즉, MSE의 관점에서 loss가 작은 것이 $p(x)$에 더 크게 관여하는데, MSE loss가 작은 이미지가 실제 의미적으로 더 가까운 이미지가 아닌 경우가 많기 때문에 올바른 방향으로 학습할 수가 없다. 

$${\parallel x - z_{bad} \parallel}^2 < {\parallel x - z_{good} \parallel}^2 \to p(x \vert g_{\theta}(z_{bad})) > p(x \vert g_{\theta}(z_{good}))$$

예를 들면 원래 고양이 이미지에서 일부분이 조금 잘린 이미지 $a$와 한 pixel씩 옆으로 이동한 이미지 $b$가 있다고 하면 $b$는 pixel만 옆으로 밀렸을 뿐 고양이 그대로 이지만 $a$는 이미지가 잘렸기 때문에 의미적으론 $b$가 $a$보다 고양이에 가까운데, MSE 관점에서는 $b$의 loss가 더 크게 나오게 된다. 

이러한 문제를 해결하기 위해 Variational Inference가 나오게 된다. 기존 prior에서 sampling을 하니 학습이 잘 안되니까 $z$ 를 prior에서 sampling하지 말고 target인 $x$ 와 유사한 sample이 나올 수 있는 이상적인 확률분포 $p(z \vert x)$ 로 부터 sampling한다. 이때 우리는 
$p(z \vert x)$ 가 무엇인지 알지 못하기 때문에, 이미 알고 있는 확률 분포(가우시안..) $q_{\phi}(z \vert x)$ 를 임의로 택하고 그것의 파라미터 $\phi$ 를 조정하여 $p(z \vert x)$ 와 유사하게 되도록 하는 것이다. 그렇게 이상적인 확률분포에 근사된 $q_{\phi}$ 를 통해서 $z$ 를 sampling하게 된다. 

$$p(z \vert x) \approx q_{\phi}(z \vert x) \sim z$$


### ELBO
지금까지의 내용을 정리하면 우리가 구하고자 하는 것은 $p(x)$였고, 이를 위해 Prior Distribution을 사용했으며, 그냥 prior에서 sampling하려니 잘 학습이 안되서 이상적인 확률분포 $p(z|x)$ 를 근사한 $q_{\phi}$를 사용하게 됐다. 이 4개간의 관계식에서 loss를 유도하는 과정에 우리가 찾아야 하는 ELBO(Evidence LowerBOund)가 나오게 된다. 

우선 $\log(p(x))$에서 시작해서 ELBO를 유도하는 과정을 정리하면 아래와 같다. 
 
$$\begin{aligned}
\log(p(x)) &= \int \log(p(x))q_{\phi}(z|x)dz 　 \leftarrow \int q_{\phi}(z|x)dz = 1 \\ 
&=\int \log\left(\frac{p(x, z)}{p(z|x)}\right)q_{\phi}(z|x)dz 　 \leftarrow p(x) = \frac{p(x, z)}{p(z|x)} \\
&=\int \log\left(\frac{p(x, z)}{q_{\phi}(z|x)}\cdot\frac{q_{\phi}(z|x)}{p(z|x)}\right)q_{\phi}(z|x)dz \\ 
&=\int \underbrace{\log\left(\frac{p(x, z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz}_ {ELBO(\phi)} + \int \underbrace{\log\left(\frac{q_{\phi}(z|x)}{p(z|x)}\right)q_{\phi}(z|x)dz}_ {KL\left(q_{\phi}(z|x) \ || \ p(z|x)\right)} \\ 
\end{aligned}$$

여기서 $KL \left(q_{\phi}(z \vert x) \ \parallel \ p(z \vert x)\right)$ term은 Kullback–Leibler divergence로 두 확률분포 간의 거리($\ge 0$)를 구한다.
우리가 원하는 건 $q_{\phi}(z \vert x)$가 $p(z \vert x)$에 최대한 가까워 져야 하므로 $KL$을 최소화 하는 $q_{\phi}(z \vert x)$의 $\phi$를 찾아야 하는데 $p(z \vert x)$를 모르기 때문에 KL을 최소화 하는 대신 $ELBO$를 최대화 하는 $\phi$를 찾으면 된다. 

$$\log(p(x)) = ELBO(\phi) + KL(\left(q_{\phi}(z|x) \ || \ p(z|x)\right)$$ 

$$q_{\phi^*}(z|x) = \underset{\phi}{\arg\max} \ ELBO(\phi)$$

$ELBO$를 최대화 하기 위해 $ELBO$ term을 다시 전개하면 다음과 같다. 

$$\begin{aligned}
ELBO(\phi) &= \int \log \left(\frac{p(x, z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz \\
&= \int \log \left(\frac{p(x|z)p(z)}{q_{\phi}(z|x)}\right)q_{\phi}(z|x)dz \\  
&= \int \log \left(p(x|z)\right)q_{\phi}(z|x)dz - \int \log \left(\frac{q_{\phi}(z|x)}{p(z)}\right)q_{\phi}(z|x)dz \\ 
&= \mathbb{E}_ {q_{\phi}(z|x)} \left[\log\left(p(x|z)\right)\right] - KL\left(q_{\phi}(z|x) \ || \ p(z)\right)
\end{aligned}$$

그래서 정리하면 $ELBO$를 최대화 하는데 있어서 $\phi$, $\theta$로 총 2개에 대한 Optimization Problem을 해결해야 하고 이는 아래와 같다. 

**Optimization Problem 1 on $\phi$: Variational Inference**

$$\log(p(x)) \ge \mathbb{E}_ {q_{\phi}(z \vert x)} \left[\log\left(p(x \vert z)\right)\right] - KL\left(q_{\phi}(z \vert x) \ \parallel \ p(z)\right) = ELBO(\phi)$$

첫 번째로 $ELBO$를 maximize하는 $\phi$를 찾는데, 이 과정이 이상적인 sampling function을 찾는 것이다.    

**Optimization Problem 2 on $\theta$: Maximum likelihood**

$$-\sum_i \log(p(x_i)) \le -\sum_i \lbrace\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] - KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right)\rbrace$$

두 번째는 이상적인 sampling function을 찾았으므로 여기서 $z$를 sampling해서 $z$로 부터 target $x$가 나오게 하는 Conditional probability $p(x \vert g_{\theta}(z))$가 최대가 되도록 하는 확률분포를 찾는 것이다.        
$\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left(p(x_i \vert g_{\theta}(z))\right)\right]$는 $q_{\phi}$에서 sampling한 $z$에 대한 $\log\left(p(x_i \vert g_{\theta}(z))\right)$를 의미한다.  

**Final Optimization Problem**

$$\underset{\phi, \theta}{\arg\min} \sum_i -\mathbb{E}_ {q_{\phi}(z \vert x_i)} \left[\log\left( p \left(x_i \vert g_{\theta}(z)\right)\right)\right] + KL\left(q_{\phi}(z \vert x_i) \ \parallel \ p(z)\right)$$

결국 위 두 Optimization Problem을 종합하는 식이 위와 같고, 이 식이 $ELBO$를 최대화하는 것과 같게 된다. 

정리해서 이상적으로 sampling을 해주는 $q_{\phi}(z \vert x)$를 Encoder, Posterior, Inference Network 등으로 부르고, sampling된 $z$로 부터 이미지를 generate 해주는 $g_{\theta}(x \vert z)$를 Decoder, Generator, Generation Network 등으로 부른다. 
