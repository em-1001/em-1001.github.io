---
title:  "MLE & MAP"
excerpt: "Maximum Likelihood Estimation, Maximum a Posterior"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

# Bayesian statistic

고전 통계학의 빈도주의적 접근은 관측된 현상(phenomena)이 '우리가 알지 못하는 고정된 프로세스'로부터 발생한 것이라 가정하고, 그 현상을 분석하는 통계학이다. 반면 베이지안 관점은 확률을 믿음(신념)의 정도로 나타낸 후, 새로운 사건을 관츰함에 따라 믿음의 정도(확률)을 갱신하는 것으로 통계에 대한 모수를 고정된 상수로 보는 빈도주의 접근과 달리 모수를 확률변수로 인식한다. 

베이지안 통계의 베이즈 정리는 다음과 같다.

$$\frac{P(H) \cdot P(Data \vert H)}{P(Data)} = P(H \vert Data)$$

$P(H \vert Data)$: 사후 확률(Posterior Probability)- Data를 고려한 후의 가설이나 사건의 갱신된 확률    
$P(Data \vert H)$: 우도(Likelihood) - 특정 가설이나 모델로 관측된 데이터를 설명할 수 있는 정도  
$P(H)$: 사전확률(Prior Probability) - Data를 고려하기 전의 사건에 대한 초기 믿음 또는 확률  

$H$는 Hypothesis로 $P(H)$는 우리가 궁금해하는 Hypothesis가 참일 확률이고, 우리의 믿음의 강도를 나타낸다. 

동전 던지기를 할 때, 느낌상 100번을 던지면 60번은 앞면이 나오는 거 같다고 치자. 그래서 동전이 Unfair하다고 생각한다면, 동전이 Unfair할 확률은 0.6이 된다. 그러면 $P(H) = P(Unfair) = 0.6$이고, Fair할 확률은 $P(Fair) = 0.4$가 된다. 여기서 동전이 Fair하다는 것은 앞면과 뒷면이 나올 확률이 0.5로 같다는 것이다. Unfair할 경우 앞면이 나올 경우는 0.6이라 하자. 

동전을 한 번 던졌을 때 앞면이 나왔을 경우, 동전이 Unfair하다는 믿음이 갱신되는 과정을 살펴보자. 

$$\begin{align}
P(Unfair) &= 0.6 \\ 
P(Data \vert Unfair) &= 0.6 \\ 
P(Data) &= 0.4 \times 0.5 + 0.6 \times 0.6 \\ 
\end{align}$$

$$P(Unfair \vert Data) = \frac{P(Unfair) \cdot P(Data \vert Unfair)}{P(Data)} = 0.6429$$

원래 0.6이었던 $P(H) = P(Unfair) = 0.6$가 Data를 반영한 뒤 갱신되어 $P(H \vert Data) = P(Unfair \vert Data) = 0.6429$로 높아진 것을 확인할 수 있다. 

동전을 두 번 던졌을 때 두 번 모두 앞면이 나왔을 경우, Unfair 믿음이 갱신되는 과정은 다음과 같다. 

$$\begin{align}
P(Unfair) &= 0.6 \\ 
P(Data \vert Unfair) &= (0.6 * 0.6) \\ 
P(Data) &= 0.4 \times (0.5 \times 0.5) + 0.6 \times (0.6 \times 0.6) \\ 
\end{align}$$

$$P(Unfair \vert Data) = \frac{P(Unfair) \cdot P(Data \vert Unfair)}{P(Data)} = 0.6835$$

두 번 모두 앞면이 나온경우 Unfair에 대한 믿음이 더욱 강해지는 것을 확인할 수 있다.     
베이지안 확률에서는 이런 식으로 데이터를 관측하면서 확률을 갱신하는 방식을 사용한다. 

# Beta distribution

$\beta$분포의 pdf는 다음과 같다. 

$$\begin{align}
C \cdot x^{a-1} (1-x)^{b-1} &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} x^{a-1} (1-x)^{b-1} \\ 
&= \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \sim \beta(a, b)
\end{align}$$

$\beta$분포는 Bernoulli나, Binomial처럼 성공/실패에 대한 이항 결과에 대한 확률 분포를 다루는 것인데, Bernoulli와 Binomial처럼 성공의 횟수가 확률 변수가 아니라 성공의 비율을 확률 변수로 다룬다. 따라서 $\beta$분포를 통해 성공의 비율에 대한 확률을 알아낼 수 있다. 

예를 들어,  Binomial의 경우 p가 주어져 있고, $X=k$번 성공 확률의 distribution은 $P(X \vert p) \sim B(n, p)$이다. 반면에 거꾸로 $a$번 성공, $b$번 실패일 때 $p$의 distribution은 $p \sim \beta(a, b)$를 따른다. 즉 관측된 데이터를 보고 $p$의 분포를 따져보는 것으로 $p$자체가 random variable이 된다. 그래프로 x축은 비율(확률)이 되고, y축은 그 비율(확률)일 확률이 된다. (x축은 0~1이다.)

$\beta$분포의 Mode(최빈값)은 성공과 실패의 비율로 결정되는데, $a$ 성공, $b$ 실패에 대해 최빈값은 다음과 같다. 

$$mode = \frac{a -1}{a + b -2}$$

<p align="center"><img src="https://github.com/user-attachments/assets/43ef9358-0759-4317-9a54-d6b171d3ff47"></p>



# MLE

# MAP

데이터 사이언티스트 되기 책: https://recipesds.tistory.com/
