---
title:  "[Statistics] Degrees of Freedom"
excerpt: "degrees of freedom"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> 데이터 사이언티스트 되기 책: https://recipesds.tistory.com/

# Sample Variance

분산은 관측값과 평균의 차이를 제곱한 값들의 합을 $n$으로 나눈다. 하지만 실제 표본분산을 구할 때는 $n$이 아닌, $n-1$로 나누고 이 $n-1$을 자유도(Degrees of Freedom)라고 부른다. 이는 표본분산을 통해 모집단의 분산인 모분산을 추정하기 위함이다. 모분산보다 표본분산이 더 작게 나와서 이를 보정하기 위해 $n-1$로 나눈다는 것이다. 이렇게 하면 "확률적"으로 모분산의 추정 값이 된다고 한다. 이렇게 계산한 추정량을 불편추정량이라 한다. 

실제로 왜 $n-1$로 나누게 되는건지 증명해보자. 첫번재 증명은 모분산과 $n$으로 나눈 표본분산의 차이를 구하는 방법을 사용한다. 
$n$크기의 모집단에서 중복을 허용하여 추출한 $n$개의 표본을 이용하여 모분산과 표본분산의 차이를 계산한다. 이때 표본분산은 $n-1$이 아니라, $n$으로 나눈 표본분산이다. 

$$\begin{align}
E \lbrack \sigma^2 - s_n^2 \rbrack &= E \lbrack \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2 - \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2 \rbrack \\ 
&= E \lbrack \frac{1}{n}\sum_{i=1}^n ((x_i^2 - 2x_i\mu + \mu^2) - (x_i^2 - 2x_i\bar{x} + \bar{x}^2)) \rbrack \\ 
&= E \lbrack \frac{1}{n}\sum_{i=1}^n (\mu^2 - \bar{x}^2 + 2x_i(\bar{x} - \mu)) \rbrack \\ 
&= E \lbrack \mu^2 - \bar{x}^2 + \frac{1}{n}\sum_{i=1}^n 2x_i(\bar{x} - \mu) \rbrack \\  
&= E \lbrack \mu^2 - \bar{x}^2 + 2(\bar{x} - \mu)\bar{x} \rbrack \\ 
&= E \lbrack \mu^2 - 2\bar{x}\mu + \bar{x}^2 \rbrack \\  
&= E \lbrack (\bar{x} - \mu)^2 \rbrack \\  
&= Var(\bar{x}) \\  
&= \frac{\sigma^2}{n} \quad \because \ {\scriptstyle\text{The variance of sample mean from Central Limit Thm.
}}
\end{align}$$

위 결과를 통해 모분산과 표본분산의 차이가 $\frac{\sigma^2}{n}$만큼 나는 것을 알 수 있다. 따라서 아래가 성립한다. 

$$E \lbrack s_n^2 \rbrack = \sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2$$  

$$\sigma^2 = \frac{n}{n-1}E\lbrack s_n^2 \rbrack = \frac{n}{n-1} \cdot \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 = \frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}$$

이처럼 표본분산을 계산할 때, $n$대신 $n-1$로 나누면 모분산과 비슷해 지는것을 알 수 있다. 

자유도 설명에 앞서 모분산보다 표본분산이 더 작게 나와서 이를 보정하기 위해 $n-1$로 나눈다 했는데, 그 이유를 설명하면 모분산은 직관적으로 다음과 같다. 

표본 값들이 모평균에서 퍼진 정도(=모분산=$\sigma^2$) = 표본값들이 표본 평균에서 퍼진 정도 + 표본 평균자체가 퍼진 정도(=표본평균의 분산=$\frac{\sigma^2}{n} \to$ 중심극한정리)

따라서 표본값들이 표본 평균에서 퍼진 정도 = $\sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2$ 가 되어 표본분산이 모분산보다 작다. 


# Degrees of Freedom

자유도(Degrees of Freedom)는 주어진 조건에서 자유롭게 변화할 수 있는(Independent) 데이터의 수이다. 
$x_1, x_2, \cdots, x_n$의 observation이 있을 때 평균을 계산하여 알게되면, 원래 observation중 한 값을 잃어도 평균값을 알고 있기 때문에 정보는 전혀 손실되지 않는다. 따라서 평균을 알게된 순간 자료 중 어느 한 값은 항상 자유롭지 않은 의미 없는 정보가 되고, 자유도는 'observation-1' 즉, $n-1$이 된다. 따라서 처음 observation에 대해 평균을 계산하는 시점에는 자유도가 $n$이고, 평균으로 분산을 계산하는 시점에는 자유도가 $n-1$이 된다. 

통계는 무엇이든 평균을 내는 것이 기본이고, 흔히 구하는 평균도 평균, 분산도 평균이다. 이 평균을 구할 때 의미를 가지고 있는 데이터의 개수로 나누는 것이 통계의 기본이다. 이때 의미있는 데이터의 수가 자유도이다. 말 그대로 자유롭게 변할 수 있는 값을 가질 수 있는 변수의 수를 말한다. 통계에서는 이러한 변수의 수(자유도)를 의미있는 데이터의 수라 한다. 통계에서는 자유도만큼의 데이터만 사용해서 통계 값을 구해야 Unbiased Estimation이 되고, 자유도를 고려하지 않고 $n$개의 데이터를 그대로 통계값을 구하는데 사용하면 Biased Estimation이 된다.

예를 들어, 표본평균을 구할 때는 $n$개의 데이터가 모두 있어야 표본평균을 구할 수 있다. 따라서 이 때의 자유도는 $n$이고, 평균을 구할 때 $n$으로 나눈다. 표본분산을 구할 때는 $n$개의 데이터와 $\hat{\mu}$가 필요한데, $\hat{\mu}$는 이미 구했고, $\hat{\mu}$가 정해지니까 $n-1$개의 데이만 있으면 나머지 1개의 데이터는 자연스럽게 알 수 있는 값이 된다. 따라서 이 때의 자유도는 $n-1$이 되어 분산을 구할 때 자유도인 $n-1$로 나누게 된다. 

일반적으로 생각하면, $n$개의 observation에서 parameter(모수)를 구할 때마다 한 개의 observation이 통계적인 의미를 잃어게 된다. 

다른 예로 Linear Regression (단순 회귀)에서 표본분산의 자유도를 따져본다면, $y=ax+b$를 Regression한다고 했을 때, 최종 자유도는 모수인 기울기와 $y$절편 즉, $a, b$ 두 개의 값을 Obervation으로 구한 다음에야 $e_i$를 계산할 수 있으니까, $e_i$를 구할 때의 최종 자유도는 $n-2$가 된다. 

$$s^2 = \frac{\sum_{i=1}^n e_i^2}{n-2} = \frac{SSE}{n-2}$$



















