---
title:  "Raum"
excerpt: "Brain Tumor Segmentation using U-Net Transformer"

categories:
  - Computer Vision
tags:
  - Project
  - Segmentation
last_modified_at: 2024-12-28T08:06:00-05:00
---


# UNETR: Transformers for 3D Medical Image Segmentation

## Vision Transformer 

<p align="center"><img src="https://github.com/user-attachments/assets/18f39354-27e4-4cce-8df0-6846851be9f5"></p>

NLP에서의 Transformer가 1D sequence of input embeddings을 입력 받기 때문에 Vision 분야에서도 유사하게 2D, 3D입력을 1D로 변환해줄 필요가 있다. UNETR에서 제안하는 3D을 기준으로 설명하면 resolution $(H, W, D)$ Channel $C$의 3D input volume $\mathbf{x} \in \mathbb{R}^{H \times W \times D \times C}$ 을 Flatten 시켜서 $\mathbf{x}_v \in \mathbb{R}^{N \times (P^3 \cdot C)}$ 으로 만든다. 여기서 $(P, P, P)$는 image patch의 resolution이고, $N = (H \times W \times D) / P^3$ 은 Patch의 수, length of the sequence이다. 

이렇게 Flatten한 Patch를 학습 가능한 Linear Projection($E$) ( $E \in \mathbb{R}^{(P^3 \cdot C) \times K}$ ) 을 통해 $K$ dimensional embedding space 으로 만든다. 이후 NLP 처럼 이미지의 위치 정보를 보존하기 위해 학습 가능한 1D positional embedding ($E_{pos} \in \mathbb{R}^{N \times K}$) 을 embedding결과에 더해준다. 

$$z_0 = [\mathbf{x}_v^1E; \mathbf{x}_v^2E; ...; \mathbf{x}_v^NE] + E_{pos}$$

여기선 semantic segmentation이 목적이기 때문에 [CLS] Token은 사용되지 않는다. 

embedding이 끝난 후 본격적으로 Transformer 네트워크를 통과한다. 여기서 부턴 NLP에서의 과정과 매우 유사하다. 

$$z_i^{\prime} = MSA(Norm(z_{i-1})) + z_{i-1}, i=1...L$$

$$z_i = MLP(Norm(z_i^{\prime})) + z_i^{\prime}, i=1...L$$

multilayer perceptron (MLP) 은 GELU activation를 사용하는 2개의 linear layers로 이루어져 있고, $L$은 transformer layers를 반복하는 수이다. 

multi-head self-attention (MSA) 은 $n$ parallel self-attention(SA) heads로 이루여져 있으며 다음과 같이 계산한다. 
query, key, value는 입력 벡터를 3배의 dimension으로 늘린 뒤 각각을 query, key, value로 넣는다. 

$$A = Softmax \left( \frac{qk^T}{\sqrt{K_h}} \right),   K_h=K/n$$

$$SA(z) = Av$$

$$MSA(z) = [SA_1(z); SA_2(z); ...; SA_n(z)]W_{msa}$$

$W_{msa} \in \mathbb{R}^{n \cdot K_h \times K}$ 는 학습가능한 파라미터 가중치이다. 


## Architecture

<p align="center"><img src="https://github.com/user-attachments/assets/0cc46698-0fff-4973-9eb1-0ae773f59eca"></p>

transformer를 활용한 다른 3D medical image segmentation 연구에서는 CNN을 feature extraction에 사용하고 Encoder와 Decoder를 잇는 bottleneck에 transformer를 두는 방식을 제안하였다. 하지만 본 논문에서는 transformer를 Encoder에 사용하고, 이를 바로 Decoder와 skip connections으로 연결하는 방법을 제안한다.   
모델에서 Transformer를 Encoder할 때만 사용하고 Decoder할 때는 CNN-based를 사용하는 이유는 Transformer가 global information는 매우 잘 잡아내지만, localized information에 대해서는 부적합하기 때문이다. 

U-net 구조와 유사하게 $\frac{H \times W \times D}{P^3} \times K$ 의 size를 갖는 transformer에서의 결과들 $z_i (i \in {3,6,9,12})$ 을 $\frac{H}{P} \times \frac{W}{P} \times \frac{D}{P} \times K$ 의 tensor로 reshape하여 decoder와 merge시킨다. 

transformer의 마지막 layer 결과인 bottleneck에서는 deconvolutional layer를 통과시켜서 resolution을 2배 증가시킨다. 이 feature map을 이전 transformer output인 $z_9$의 feature map에 Deconv를 통과시킨 결과와 concat하고, $3 \times 3 \times 3$ conv에 통과시킨 뒤, 다시 upsample을 한다. 이러한 과정을 원래 input resolution까지 반복하고 최종적으로 $1 \times 1 \times 1$ conv와 softmax에 통과시켜 voxel-wise semantic predictions을 얻는다. 

## Loss Function 

본 논문에서는 Loss Function으로 dice loss와 cross-entropy loss를 사용한다. 

$$\mathcal{L}(G, Y) = 1 - \frac{2}{J} \sum^J_{j=1} \frac{\sum^I_{i=1} G_{i,j}Y_{i,j}}{\sum^I_{i=1}G^2_{i,j} + \sum^I_{i=1}Y^2_{i,j}} - \frac{1}{I}\sum^I_{i=1}\sum^J_{j=1}G_{i,j} \log Y_{i,j}$$

- $I$: number of voxels
- $J$: number of classes
- $Y_{i,j}$: probability output for class $j$ at voxel $i$
- $G_{i,j}$: ground truth for class $j$ at voxel $i$




# Project: Brain Tumor Segmentation using UNETR

프로젝트 진행중 ... 
