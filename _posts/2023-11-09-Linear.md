---
title:  "[Statistics] Linear Algebra"
excerpt: Basic Linear Algebra

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> 데이터 사이언티스트 되기 책: https://recipesds.tistory.com/


# Linear Algebra

다음 통계학 개념들을 설명하기 전에 필요한 기초적인 선형대수 내용을 다음 4가지로 다룰 것이다. 

1. 행렬은 변환이다.
2. 행렬의 Eigen Value, Eigen Vector는 행렬의 변환에 대한 항등원이다.
3. 행렬의 변환에 의한 기존 좌표계의 기저Basis 변환
4. 행렬은 그 자체로 데이터를 설명한다.


**1. 행렬은 변환이다.**  

우선 행렬의 곱은 다른 측면에서 보면 벡터(입력)의 선형 변환이다. 아래 예시는 어떤 행렬에 (1,1)을 넣으면 (3,7)로 변환되는 예이다. 

<p align="center"><img src="https://github.com/user-attachments/assets/0b9ce40d-8cb7-4dbd-9a46-f85e3763aeae" height="" width=""></p>

(1,1)벡터 1개의 예를 보면 위와 같은데 가로 0~1, 세로 0~1범위의 사각형에 있는 데이터들을 이 행렬을 통해 변화시키면 아래와 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/96d9e10e-d3c2-4910-8f27-66176ea663ce" height="" width=""></p>


**2. 행렬의 Eigen Value, Eigen Vector는 행렬의 변환에 대한 항등원이다.**

먼저 Eigen Value, Eigen Vector에 대해 살펴보자. 선형 변환을 하는 행렬 A가 다음과 같다고 하자. 

$$A = \begin{vmatrix}
2 & 1 \\ 
3 & 4
\end{vmatrix}$$

Eigen Vector는 수식으로 정의할 때 다음과 같이 정의한다. 

$$Ax = \lambda x$$

이러한 관계를 만족하면 $x$를 A에 대한 Eigen Vector라 하고 $\lambda$를 Eigen Value라고 한다. 의미를 해석하면 A변환에 대해서 어떤 x vector는 A로 변환했을 때, 
$\lambda$배만큼 크기만 변하고 벡터 자체는 변하지 않는다는 뜻이다. 

Eigen Vector와 Eigen Value를 구하는 방법은 다음과 같다. 

$$
Ax = \lambda x \\  
Ax = \lambda I x \\  
Ax - \lambda I x = 0 \\   
(A - \lambda I)x = 0
$$

이때 만약 $(A - \lambda I)$가 역행렬을 갖는다면 $x = (A - \lambda I)^{-1} 0 = 0$가 되므로 $x$ 벡터는 0벡터가 될 수 밖에 없다. 
따라서 $x$는 0이 아닌 해를 가져야 하니가 $(A - \lambda I)$는 역행렬을 가지면 안된다. 역행렬을 갖지 않는 조건은 $det \vert (A - \lambda I) \vert = 0$이므로 이 식을 풀어서 $\lambda$를 구할 수 있고, 그 $\lambda$에 어울리는 벡터를 구할 수 있다. 이런 경우 $\lambda$는 유일하고, 그것을 만족하는 벡터 $x$는 무수히 많다. 

$$\begin{align}
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \lambda \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \lambda \begin{vmatrix} 1 & 0 \\ 0 & 1\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \begin{vmatrix} \lambda & 0 \\ 0 & \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} - \begin{vmatrix} \lambda & 0 \\ 0 & \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
&\begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\ 
&det \left( \begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} - \lambda \begin{vmatrix} 1 & 0 \\ 0 & 1\end{vmatrix} \right) = 0 \\  
&det \begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} = (2 - \lambda)(4 - \lambda) - 1 \cdot 3 = 0 \\  
&\lambda^2 - 6\lambda + 5 = 0 \\  
&(\lambda - 1)(\lambda - 5) = 0 \\  
&\therefore \lambda = 1 \ or \ 5
\end{align}$$

위 예시로 Eigen Value가 1이거나 5인 것을 찾아냈다. 이제 아래 식에 Eigen Value를 넣어서 그 식을 만족하는 $(x_1, x_2)$ 벡터를 구하면 그 고유값의 고유 벡터인 Eigen Vector가 된다. 

$$\begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0$$

1) $\lambda = 1

$$
\begin{vmatrix} 2 - 1 & 1 \\ 3 & 4 - 1\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
x_1 + x_2 = 0 \\  
x_1 = -x_2 \\  
\therefore (1, -1)
$$

수 많은 $x_1, x_2$ 중 가장 다루기 편한 크기의 벡터로 정해 보면 $(1, -1)$이다. 

2) $\lambda = 5

$$
\begin{vmatrix} 2 - 5 & 1 \\ 3 & 4 - 5\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
-3x_1 + x_2 = 0 \\  
3x_1 = x_2 \\  
\therefore (1, 3)
$$

이렇게 Eigen Value (1, 5) / Eigen Vector ((1, -1), (1, 3))의 pair를 찾았다. 처음의 정의를 다시 보면 어떤 변환 행렬 A에 대하여 (1, -1)과 (1, 3)은 $\lambda$만큼 크기만 바뀌고 방향은 그대로이다. Eigen Vector는 행렬 A에 의해 방향이 변환이 되지 않는 항등원 같은 Vector인 셈이다. 

Gaussian Random Noise Data를 만들어서 행렬 A를 이용해 변환해보면 다음과 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/582b8cdf-fa51-49c2-a520-e55ac2860bcd" height="" width=""></p>

위 가우시안 노이즈를 변환하면 아래와 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/27ac293a-6ba5-4241-8717-087c1e9d84c7" height="" width=""></p>

이것이 어떤 의미를 갖는가 하면, A를 통해서 데이터를 변환하게 되면 Eigen Vector들의 방향으로 데이터들이 늘어나서 자리하게 되는데, 이 방향이 Eigen Vector 방향에 Eigen Value 크기 방향이다. 


**3. 행렬의 변환에 의한 기존 좌표계의 기저 Basis 변환**

행렬의 또 다른 측면은 기저 basis의 변환이다. 입력 벡터가 다음과 같다고 할 때, 

$$\begin{vmatrix} x_1 \\ x_2\end{vmatrix}$$

이 입력 벡터는 아래와 같이 (1, 0), (0, 1) 기저의 선형 조합으로 다시 쓸 수 있다. 

$$\begin{vmatrix} x_1 \\ x_2\end{vmatrix} = x_1 \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 \begin{vmatrix} 0 \\ 1\end{vmatrix}$$

그렇다면 행렬 A에 대해 다음과 같이 다시 쓸 수 있다. 

$$A \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = A \left( x_1 \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 \begin{vmatrix} 0 \\ 1\end{vmatrix} \right) = x_1 A \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 A \begin{vmatrix} 0 \\ 1\end{vmatrix}$$

이는 수평방향의 기저 (1, 0)를 A 변환한 후 $x_1$배, 수직방향의 기저 (0, 1)를 A 변환한 후 $x_2$배 한 것과 같다. 즉, 수평, 수직 방향의 기저를 변환한 것이다. 





