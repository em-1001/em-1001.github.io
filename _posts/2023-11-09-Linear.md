---
title:  "[Statistics] PCA(Principal Component Analysis)"
excerpt: Basic Linear Algebra, Principal Component Analysis

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> 데이터 사이언티스트 되기 책: https://recipesds.tistory.com/


# Linear Algebra

다음 통계학 개념들을 설명하기 전에 필요한 기초적인 선형대수 내용을 다음 4가지로 다룰 것이다. 

1. 행렬은 변환이다.
2. 행렬의 Eigen Value, Eigen Vector는 행렬의 변환에 대한 항등원이다.
3. 행렬의 변환에 의한 기존 좌표계의 기저Basis 변환
4. 행렬은 그 자체로 데이터를 설명한다.


**1. 행렬은 변환이다.**  

우선 행렬의 곱은 다른 측면에서 보면 벡터(입력)의 선형 변환이다. 아래 예시는 어떤 행렬에 (1,1)을 넣으면 (3,7)로 변환되는 예이다. 

<p align="center"><img src="https://github.com/user-attachments/assets/0b9ce40d-8cb7-4dbd-9a46-f85e3763aeae" height="" width=""></p>

(1,1)벡터 1개의 예를 보면 위와 같은데 가로 0~1, 세로 0~1범위의 사각형에 있는 데이터들을 이 행렬을 통해 변화시키면 아래와 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/96d9e10e-d3c2-4910-8f27-66176ea663ce" height="" width=""></p>


**2. 행렬의 Eigen Value, Eigen Vector는 행렬의 변환에 대한 항등원이다.**

먼저 Eigen Value, Eigen Vector에 대해 살펴보자. 선형 변환을 하는 행렬 A가 다음과 같다고 하자. 

$$A = \begin{vmatrix}
2 & 1 \\ 
3 & 4
\end{vmatrix}$$

Eigen Vector는 수식으로 정의할 때 다음과 같이 정의한다. 

$$Ax = \lambda x$$

이러한 관계를 만족하면 $x$를 A에 대한 Eigen Vector라 하고 $\lambda$를 Eigen Value라고 한다. 의미를 해석하면 A변환에 대해서 어떤 x vector는 A로 변환했을 때, 
$\lambda$배만큼 크기만 변하고 벡터 자체는 변하지 않는다는 뜻이다. 

Eigen Vector와 Eigen Value를 구하는 방법은 다음과 같다. 

$$
Ax = \lambda x \\  
Ax = \lambda I x \\  
Ax - \lambda I x = 0 \\   
(A - \lambda I)x = 0
$$

이때 만약 $(A - \lambda I)$가 역행렬을 갖는다면 $x = (A - \lambda I)^{-1} 0 = 0$가 되므로 $x$ 벡터는 0벡터가 될 수 밖에 없다. 
따라서 $x$는 0이 아닌 해를 가져야 하니가 $(A - \lambda I)$는 역행렬을 가지면 안된다. 역행렬을 갖지 않는 조건은 $det \vert (A - \lambda I) \vert = 0$이므로 이 식을 풀어서 $\lambda$를 구할 수 있고, 그 $\lambda$에 어울리는 벡터를 구할 수 있다. 이런 경우 $\lambda$는 유일하고, 그것을 만족하는 벡터 $x$는 무수히 많다. 

$$\begin{align}
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \lambda \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \lambda \begin{vmatrix} 1 & 0 \\ 0 & 1\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = \begin{vmatrix} \lambda & 0 \\ 0 & \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} \\  
&\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} - \begin{vmatrix} \lambda & 0 \\ 0 & \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
&\begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\ 
&det \left( \begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix} - \lambda \begin{vmatrix} 1 & 0 \\ 0 & 1\end{vmatrix} \right) = 0 \\  
&det \begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} = (2 - \lambda)(4 - \lambda) - 1 \cdot 3 = 0 \\  
&\lambda^2 - 6\lambda + 5 = 0 \\  
&(\lambda - 1)(\lambda - 5) = 0 \\  
&\therefore \lambda = 1 \ or \ 5
\end{align}$$

위 예시로 Eigen Value가 1이거나 5인 것을 찾아냈다. 이제 아래 식에 Eigen Value를 넣어서 그 식을 만족하는 $(x_1, x_2)$ 벡터를 구하면 그 고유값의 고유 벡터인 Eigen Vector가 된다. 

$$\begin{vmatrix} 2 - \lambda & 1 \\ 3 & 4 - \lambda\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0$$

1) $\lambda = 1$

$$
\begin{vmatrix} 2 - 1 & 1 \\ 3 & 4 - 1\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
x_1 + x_2 = 0 \\  
x_1 = -x_2 \\  
\therefore (1, -1)
$$

수 많은 $x_1, x_2$ 중 가장 다루기 편한 크기의 벡터로 정해 보면 $(1, -1)$이다. 

2) $\lambda = 5$

$$
\begin{vmatrix} 2 - 5 & 1 \\ 3 & 4 - 5\end{vmatrix} \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = 0 \\  
-3x_1 + x_2 = 0 \\  
3x_1 = x_2 \\  
\therefore (1, 3)
$$

이렇게 Eigen Value (1, 5) / Eigen Vector ((1, -1), (1, 3))의 pair를 찾았다. 처음의 정의를 다시 보면 어떤 변환 행렬 A에 대하여 (1, -1)과 (1, 3)은 $\lambda$만큼 크기만 바뀌고 방향은 그대로이다. Eigen Vector는 행렬 A에 의해 방향이 변환이 되지 않는 항등원 같은 Vector인 셈이다. 

Gaussian Random Noise Data를 만들어서 행렬 A를 이용해 변환해보면 다음과 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/582b8cdf-fa51-49c2-a520-e55ac2860bcd" height="" width=""></p>

위 가우시안 노이즈를 변환하면 아래와 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/27ac293a-6ba5-4241-8717-087c1e9d84c7" height="" width=""></p>

이것이 어떤 의미를 갖는가 하면, A를 통해서 데이터를 변환하게 되면 Eigen Vector들의 방향으로 데이터들이 늘어나서 자리하게 되는데, 이 방향이 Eigen Vector 방향에 Eigen Value 크기 방향이다. 


**3. 행렬의 변환에 의한 기존 좌표계의 기저 Basis 변환**

행렬의 또 다른 측면은 기저 basis의 변환이다. 입력 벡터가 다음과 같다고 할 때, 

$$\begin{vmatrix} x_1 \\ x_2\end{vmatrix}$$

이 입력 벡터는 아래와 같이 (1, 0), (0, 1) 기저의 선형 조합으로 다시 쓸 수 있다. 

$$\begin{vmatrix} x_1 \\ x_2\end{vmatrix} = x_1 \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 \begin{vmatrix} 0 \\ 1\end{vmatrix}$$

그렇다면 행렬 A에 대해 다음과 같이 다시 쓸 수 있다. 

$$A \begin{vmatrix} x_1 \\ x_2\end{vmatrix} = A \left( x_1 \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 \begin{vmatrix} 0 \\ 1\end{vmatrix} \right) = x_1 A \begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2 A \begin{vmatrix} 0 \\ 1\end{vmatrix}$$

이는 수평방향의 기저 (1, 0)를 A 변환한 후 $x_1$배, 수직방향의 기저 (0, 1)를 A 변환한 후 $x_2$배 한 것과 같다. 즉, 수평, 수직 방향의 기저를 변환한 것이다. 

예를 들면 다음과 같다. 

$$\begin{align}
&= x_1\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix}\begin{vmatrix} 1 \\ 0\end{vmatrix} + x_2\begin{vmatrix} 2 & 1 \\ 3 & 4\end{vmatrix}\begin{vmatrix} 0 \\ 1\end{vmatrix} \\ 
&= x_1\begin{vmatrix} 2 \\ 3\end{vmatrix} + x_2\begin{vmatrix} 1 \\ 4\end{vmatrix}
\end{align}$$

(1, 0)이 (2, 3)으로, (0, 1)이 (1, 4)로 변경되었다. 즉, (2, 3)와 (1, 4)는 A에 의해 변횐되는 벡터의 기저가 된다는 것이다. 

<p align="center"><img src="https://github.com/user-attachments/assets/3f096920-1f3b-4e3e-995c-7b415283b891" height="" width=""></p>

자세히 보면 새로운 기저는 행렬의 각 column이다. 이렇게 basis가 새로운 basis로 변형되면 새로운 기저의 선형합으로 나타낼 수 있는 공간이 새로 생긴다. 

<p align="center"><img src="https://github.com/user-attachments/assets/7b29aeb4-06eb-4efb-9fbc-25346be69567" height="" width=""></p>

이 새로운 공간에 대한 모든 정보를 행렬 A가 갖고 있다. 

<p align="center"><img src="https://github.com/user-attachments/assets/7f05a9e8-b2e0-46fa-89e5-f19229640e53" height="" width=""></p>

결국 위와 같은 모양으로 모두 (1, 4), (2, 3)의 선형 조합 Linear Combination으로 표현이 가능한 데이터들이다. 


**4. 행렬은 그 자체로 데이터를 설명한다.**

지금까지의 내용으로 행렬은 어떤 벡터 데이터를 변환시키는 일을 하는데, 결국에는 행렬과 관계된 벡터의 분포 즉, 모양새를 이미 설명하고 있다. 

이제까지 살펴본 행렬 A에 대해 A로 인해 가질 수 있는 데이터의 모양새는 새로운 기저와 Eigen Vector, Eigen Value이다. 
이때 고유값 Eigen Value 중 가장 큰 값을 갖는 것을 Dominant 하다고 한다. 이런 식으로 새로운 basis와 Eigen Vector를 합쳐서 보면 행렬이 데이터를 설명한다고 할 수 있다. 

예를 들어 가로축 0~1, 새로축 0~3으로 uniform distributed 한 데이터를 변환하면 다음과 같다.

<p align="center"><img src="https://github.com/user-attachments/assets/2f251dbf-fd08-43ec-b1de-87bbcccb1a5e" height="" width=""></p>

다른 특별한 예를 들어 모든 element가 양수이고, 대칭 행렬(Symmetry)인 경우의 행렬을 양의 정부호 행렬이라 하는데 다음과 같다. 

$$\begin{vmatrix} 2 & 1 \\ 1 & 2\end{vmatrix}$$

이제 Eigen Value와 Eigen Vector를 구해보면 Eigen Value는 1, 3이 된다. 이때의 각각 Eigen Vector는 (-1, 1), (1, 1)이다. 여기에 벡터의 크기를 1로 만드는 정규화를 하면 다음과 같다. 

$$x_1 = \frac{1}{\sqrt{2}}\begin{vmatrix} -1 \\ 1\end{vmatrix}, x_2 = \frac{1}{\sqrt{2}}\begin{vmatrix} 1 \\ 1\end{vmatrix}$$

Eigen Value가 3일 때가 Dominant Eigen Vector이다. 이 행렬에 대한 변환을 그림으로 표현하면 다음과 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/9b4b8426-edfc-4520-8c95-628a55a0bae4" height="" width=""></p>

이렇게 데이터와 행렬을 같이 보면 새로운 기저, Eigen Value와 Vector를 확인할 수 있다. 이렇듯 행렬이 데이터를 설명한다는 것은 행렬을 보고 변환된 데이터를 상상할 수 있다는 의미로 받아들일 수 있다. 

추가적으로 Determinant에 대해 조금 설명하면 어떤 행렬의 Determinant는 행렬의 Basis를 변으로 갖는 평행사변형의 면적이다. 이 면적으로 행렬의 크기를 정해서 사용하는데, 행렬의 절대값이라 할 수 있다. Determinant의 절대값은 선형 변환의 크기를 나타내고, 부호는 변환되는 데이터의 모양의 보존 여부를 의미한다. 즉, 다음과 같다. 

변환된 면적 = $\vert det(A) \vert$ 변환 전 면적 

Determinant가 0이라는 의미는 이 면적이 0이라는 뜻이다. 이것은 행렬을 구성하는 열 벡터가 같은 선상에 있다는 의미이다. 

<p align="center"><img src="https://github.com/user-attachments/assets/f7d0d08d-017e-46b1-8c6a-05f9e24d4b3b" height="" width=""></p>

이렇게 되면 이 행렬은 무엇을 변환시키더라도 같은 선상에 있을 수 밖에 없다. 

# PCA(Principal Component Analysis)









