---
title:  "YOLOv4"
excerpt: "YOLOv4 from scratch"

categories:
  - Project
tags:
  - AI
  - Computer Vision
  - Project
last_modified_at: 2024-09-29T08:06:00-05:00
---

<img src="/assets/images/yolov4/cat0_1.png">&#160;&#160;&#160;&#160;<img src="/assets/images/yolov4/cat1_1.png"> 

### Configuration  
```ini
DATASET = PASCAL_VOC
ANCHORS = [
    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],
    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],
    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],
] 

BATCH_SIZE = 32
OPTIMIZER = Adam
NUM_EPOCHS = 100
CONF_THRESHOLD = 0.05
MAP_IOU_THRESH = 0.5
NMS_IOU_THRESH = 0.45
WEIGHT_DECAY = 1e-4

# 0 ~ 30 epoch                # Cosine Annealing                            

LEARNING_RATE = 0.0001        LEARNING_RATE = 0.0001        
                              T_max = 100
# 30 ~ 50 epoch               

LEARNING_RATE = 0.00005       

# 50 ~  epoch                

LEARNING_RATE = 0.00001      

```

### NMS(Non-maximum Suppression)

|Detection|320 x 320|416 x 416|512 x 512|
|--|--|--|--|
|CSP|?|43.5|?|
|CSP + GIoU|?|?|?|
|CSP + DIoU|?|?|?|
|CSP + CIoU|?|46.4|?|
|CSP + SIoU|?|46.2|?|
|CSP + CIoU + CA|?|?|?|
|CSP + CIoU + CA + M|?|?|?|

### DIoU-NMS

|Detection|320 x 320|416 x 416|512 x 512|
|--|--|--|--|
|CSP + CIoU|?|46.4|?|
|CSP + CIoU + CA|?|?|?| 


# YOLOv3

## Bounding Box

<p align="center"><img src="/assets/images/yolov4/bbox.png"></p>

YOLOv2 부터 Anchor box(prior box)를 미리 설정하여 최종 bounding box 예측에 활용한다. 위 그림에서는 $b_x, b_y, b_w, b_h$가 최종적으로 예측하고자 하는 bounding box이다. 검은 점선은 사전에 설정된 Anchor box로 이 Anchor box를 조정하여 파란색의 bounding box를 예측하도록 한다.  

모델은 직접적으로 $b_x, b_y, b_w, b_h$ 를 예측하지 않고 $t_x, t_y, t_w, t_h$를 예측하게 된다.
범위제한이 없는 $t_x, t_y$ 에 sigmoid($\sigma$)를 적용해주어 0과 1사의 값으로 만들어주고, 이를 통해 bbox의 중심 좌표가 1의 크기를 갖는 현재 cell을 벗어나지 않도록 해준다. 여기에 offset인 $c_x, c_y$를 더해주면 최종적인 bbox의 중심 좌표를 얻게 된다.

$b_w, b_h$의 경우 미리 정해둔 Anchor box의 너비와 높이를 얼만큼의 비율로 조절할 지를 Anchor와 $t_w, t_h$에 대한 log scale을 이용해 구한다.

YOLOv2에서는 bbox를 예측할 때 $t_x, t_y, t_w, t_h$를 예측한 후 그림에서의 $b_x, b_y, b_w, b_h$로 변형한 뒤 $L_2$ loss를 통해 학습시켰지만, YOLOv3에서는 ground truth의 좌표를 거꾸로 $\hat{t}_ {∗}$로 변형시켜 예측한 $t_{∗}$와 직접 $L_1$ loss로 학습시킨다. ground truth의 $x, y$좌표의 경우 아래와 같이 변형되고,

$$
\begin{aligned}&b_{∗}= \sigma(\hat{t}_ {∗}) + c_{∗}\\      &\sigma(\hat{t}_ {∗}) = b_{∗} - c_{∗}\\      &\hat{t}_ {∗} = \sigma^{-1}(b_{∗} - c_{∗})\end{aligned}
$$

$w, h$는 아래와 같이 변형된다.

$$
\hat{t}_ {∗} = \ln\left(\frac{b_{∗}}{p_{∗}}\right)
$$

결과적으로 $x, y, w, h$ loss는 ground truth인 $\hat{t}_ {∗}$ prediction value인 ${t}_ {∗}$사이의 차이 $\hat{t}_ {∗} - {t}_ {∗}$를 통한 Sum-Squared Error(SSE)로 구해진다.

## Model

<p align="center"><img src="/assets/images/yolov4/darknet53.png" height="35%" width="35%">    <img src="/assets/images/yolov4/model1.png" height="55%" width="55%"></p>

모델의 backbone은 $3 \times 3$, $1 \times 1$ Residual connection을 사용하면서 최종적으로 53개의 conv layer를 사용하는 **Darknet-53** 을 이용한다. Darknet-53의 Residual block안에서도 bottleneck 구조를 사용하며, input의 channel을 중간에 반으로 줄였다가 다시 복구시키는데 이렇게 하므로써 연산량을 줄일 수 있다. 이때 Residual block의 $1 \times 1$ conv는 $s=1, p=0$ 이고, $3 \times 3$ conv는 $s=1, p=1$이다.

YOLOv3 model의 특징은 물체의 scale을 고려하여 3가지 크기의 output이 나오도록 FPN과 유사하게 설계하였다는 것이다. 오른쪽 그림과 같이 $416 \times 416$의 크기를 feature extractor로 받았다고 하면, feature map이 크기가 $52 \times 52$, $26 \times 26$, $13 \times 13$이 되는 layer에서 각각 feature map을 추출한다.

<p align="center"><img src="/assets/images/yolov4/model2.png"></p>


그 다음 가장 높은 level, 즉 해상도가 가장 낮은 feature map부터 $1 \times 1$, $3 \times 3$ conv layer로 구성된 작은 Fully Convolutional Network(FCN)에 입력한다. 이후 이 FCN의 output channel이 512가 되는 시점에서 feature map을 추출한 뒤, $2\times$로 upsampling을 진행한다. 이후 바로 아래 level에 있는 feature map과 concatenate를 해주고, 이렇게 만들어진 merged feature map을 다시 FCN에 입력한다. 이 과정을 다음 level에도 똑같이 적용해주고 이렇게 3개의 scale을 가진 feature map이 만들어진다. 각 scale에 따라 나오는 최종 feature map의 형태는 $N \times N \times \left[3 \cdot (4+1+80)\right]$이다. 여기서 $3$은 grid cell당 predict하는 anchor box의 수를, $4$는 bounding box offset $(x, y, w, h)$, $1$은 objectness prediction, $80$은 class의 수 이다. 따라서 최종적으로 얻는 feature map은 $\left[52 \times 52 \times 255\right], \left[26 \times 26 \times 255\right], \left[13 \times 13 \times 255\right]$이다.

이러한 방법을 통해 더 높은 level의 feature map으로부터 fine-grained 정보를 얻을 수 있으며, 더 낮은 level의 feature map으로부터 더 유용한 semantic 정보를 얻을 수 있다.

## Loss Function

$$
λ_ {coord} \sum_ {i=0}^{S^2} \sum_ {j=0}^B 𝟙^{obj}_ {i j} \left[(t_ {x_ i} - \hat{t_ {x_ i}})^2 + (t_ {y_ i} - \hat{t_ {y_ i}})^2 \right] \\ +λ_ {coord} \sum_ {i=0}^{S^2} \sum_ {j=0}^B 𝟙^{obj}_ {i j} \left[(t_ {w_ i} - \hat{t_ {w_ i}})^2 + (t_ {h_ i} - \hat{t_ {h_ i}})^2 \right] \\ +\sum_{i=0}^{S^2} \sum_{j=0}^B 𝟙^{obj}_{i j} \left[-(o_i\log(\hat{o_i}) + (1 - o_i)\log(1 - \hat{o_i}))\right] \\ +Mask_{ig} \cdot λ_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^B 𝟙^{noobj}_{i j} \left[-(o_i\log(\hat{o_i}) + (1 - o_i)\log(1 - \hat{o_i}))\right] \\ +\sum_{i=0}^{S^2} \sum_{j=0}^B 𝟙^{obj}_ {i j} \sum_{c \in classes} \left[-(c_i\log(\hat{c_i}) + (1 - c_i)\log(1 - \hat{c_i}))\right]
$$

$S$ : number of cells

$B$ : number of anchors

$o$ : objectness

$c$ : class label

$λ_ {coord}$ : coordinate loss balance constant

$λ_{noobj}$ : no confidence loss balance constant

$𝟙^{obj}_ {i j}$ : 1 when there is object, 0 when there is no object

$𝟙^{noobj}_ {i j}$ : 1 when there is no object, 0 when there is object

$Mask_{ig}$ : tensor that masks only the anchor with iou $\le$ 0.5. Have a shape of $\left[S, S, B\right]$.

각각의 box는 multi-label classification을 하게 되는데 논문에서는 softmax가 성능이 좋지 못하기 때문에, binary cross-entropy loss를 사용했다고 한다. 하나의 box안에 복수의 객체가 존재하는 경우 softmax는 적절하게 객체를 알아내지 못하기 때문에, box 안에 각 class가 존재하는 여부를 확인하는 binary cross-entropy가 보다 적절하다고 할 수 있다.

$o$ (objectness)는 anchor와 bbox의 iou가 가장 큰 anchor의 값이 1, 그렇지 않은 경우의 값이 0인 $\left[N, N, 3, 1\right]$의 tensor로 만들어진다. $c$ (class label)은 one-encoding으로 $\left[N, N, 3, n \right]$ ($n$ : num_classes) 의 shape를 갖는 tensor로 만들어진다.

# YOLOv4

## Model

<p align="center"><img src="/assets/images/yolov4/CSPDarknet53.png" height="60%" width="60%"></p>

전체적인 구조는 YOLOv3과 유사하지만 YOLOv4는 **CSPDarknet53+SPP**를 사용한다. CSPDarknet53은 Darknet53에 CSPNet을 적용한 것이다. CSPNet은 위 사진의 CSP Residual 부분과 같이 base layer의 feature map을 두 개로 나눈 뒤($X_0 \to X_0^{'}, X_0^{''}$) $X_0^{''}$는 Dense Layer에 통과 시키고 $X_0^{'}$는 그대로 가져와서 마지막에 Dense Layer의 출력값인 ($X_0^{''}, x_1, x_2, ...$)을 transition layer에 통과시킨 $X_T$와 concat시킨다. 이후 concat된 결과가 다음 transition layer를 통과하면서 $X_U$가 생성된다.

$$
\begin{aligned}X_k &= W_K^{ * }[X_0^{''}, X_1, ..., X_{k-1}]\\  X_T &= W_T^{ * }[X_0^{''}, X_1, ..., X_{k}]\\    X_U &= W_U^{ * }[X_0^{'}, X_T]\\      \end{aligned}  
$$

$$
\begin{aligned}W_k^{'} &= f(W_k, g_0^{''}, g_1, g_2, ..., g_{k-1})\\  W_T^{'} &= f(W_T, g_0^{''}, g_1, g_2, ..., g_{k})\\  W_U^{'} &= f(W_U, g_0^{'}, g_T)\\      \end{aligned}
$$

이렇게 하므로써 CSPDenseNet은 DenseNet의 feature reuse 특성을 활용하면서, gradient flow를 truncate($X_0 \to X_0^{'}, X_0^{''}$)하여 과도한 양의 gradient information 복사를 방지할 수 있다.

## Box Loss

일반적으로 IoU-based loss는 다음과 같이 표현된다.

$$
L = 1 - IoU + \mathcal{R}(B, B^{gt})
$$

여기서 $R(B, B^{gt})$는  predicted box $B$와 target box $B^{gt}$에 대한 penalty term이다.

$1 - IoU$로만 Loss를 구할 경우 box가 겹치지 않는 case에 대해서 어느 정도의 오차로 교집합이 생기지 않은 것인지 알 수 없어서 gradient vanishing 문제가 발생했다. 이러한 문제를 해결하기 위해 penalty term을 추가한 것이다.

### **Generalized-IoU(GIoU)**

Generalized-IoU(GIoU) 의 경우 Loss는 다음과 같이 계산된다.

$$
L_{GIoU} = 1 - IoU + \frac{|C - B ∪ B^{gt}|}{|C|}
$$

여기서 $C$는 $B$와 $B^{gt}$를 모두 포함하는 최소 크기의 Box를 의미한다. Generalized-IoU는 겹치지 않는 박스에 대한 gradient vanishing 문제는 개선했지만 horizontal과 vertical에 대해서 에러가 크다. 이는 target box와 수평, 수직선을 이루는 Anchor box에 대해서는 $\vert C - B ∪ B^{gt} \vert$가 매우 작거나 0에 가까워서 IoU와 비슷하게 동작하기 때문이다. 또한 겹치지 않는 box에 대해서 일단 predicted box의 크기를 매우 키우고 IoU를 늘리는 동작 특성 때문에 수렴 속도가 느리다.

### **Distance-IoU(DIoU)**

GIoU가 면적 기반의 penalty term을 부여했다면, DIoU는 거리 기반의 penalty term을 부여한다.
DIoU의 penalty term은 다음과 같다.

$$
\mathcal{R}_{DIoU} = \frac{\rho^2(b, b^{gt})}{c^2}
$$

$\rho^2$는 Euclidean거리이며 $c$는 $B$와 $B^{gt}$를 포함하는 가장 작은 Box의 대각선 거리이다.

<p align="center"><img src="/assets/images/yolov4/diou.png" height="25%" width="25%"></p>


DIoU Loss는 두 개의 box가 완벽히 일치하면 0, 매우 멀어지면 $L_{GIoU} = L_{DIoU} \to 2$가 된다. 이는 IoU가 0이 되고, penalty term이 1에 가깝게 되기 때문이다. Distance-IoU는 두 box의 중심 거리를 직접적으로 줄이기 때문에 GIoU에 비해 수렴이 빠르고, 거리기반이므로 수평, 수직방향에서 또한 수렴이 빠르다.

**DIoU-NMS**

DIoU를 NMS(Non-Maximum Suppression)에도 적용할 수 있다. 일반적인 NMS의 경우 이미지에서 같은 class인 두 물체가 겹쳐있는 Occlusion(가림)이 발생한 경우 올바른 박스가 삭제되는 문제가 발생하는데, DIoU를 접목할 경우 두 박스의 중심점 거리도 고려하기 때문에 target box끼리 겹쳐진 경우에도 robust하게 동작할 수 있다.

$$
s_i =\begin{cases}s_ i, & IoU - \mathcal{R}_ {DIoU}(\mathcal{M}, B_i) < \epsilon\\0, & IoU - \mathcal{R}_{DIoU}(\mathcal{M}, B_i) \ge \epsilon\end{cases}
$$

가장 높은 Confidence score를 갖는 $\mathcal{M}$에 대해 IoU와 DIoU의 distance penalty를 동시에 고려하여 IoU가 매우 크더라도 중심점 사이의 거리가 멀면 다른 객체를 탐지한 것일 수도 있으므로 위와 같이 일정 임계치 $\epsilon$ 보다 작으면 없애지 않고 보존한다.

### **Complete-IoU(CIoU)**

DIoU, CIoU를 제안한 논문에서 말하는 성공적인 Bounding Box Regression을 위한 3가지 조건은 overlap area, central point
distance, aspect ratio이다. 이 중 overlap area, central point는 DIoU에서 이미 고려했고 여기에 aspect ratio를 고려한 penalty term을 추가한 것이 CIoU이다. CIoU penalty term는 다음과 같이 정의된다.

$$
\mathcal{R}_{CIoU} = \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v \\ v = \frac{4}{π^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}})^2 \\ \alpha = \frac{v}{(1 - IoU) + v}
$$

$v$의 경우 bbox는 직사각형이고 $\arctan{\frac{w}{h}} = \theta$이므로 $\theta$의 차이를 통해 aspect ratio를 구하게 된다. 이때 $v$에 $\frac{2}{π}$가 곱해지는 이유는 $\arctan$ 함수의 최대치가 $\frac{2}{π}$ 이므로 scale을 조정해주기 위해서이다.

$\alpha$는 trade-off 파라미터로 IoU가 큰 box에 대해 더 큰 penalty를 주게 된다.

CIoU에 대해 최적화를 수행하면 아래와 같은 기울기를 얻게 된다. 이때, $w, h$는 모두 0과 1사이로 값이 작아 gradient explosion을 유발할 수 있다. 따라서 실제 구현 시에는 $\frac{1}{w^2 + h^2} = 1$로 설정한다.

$$
\frac{\partial v}{\partial w} = \frac{8}{π^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{h}{w^2 + h^2} \\ \frac{\partial v}{\partial h} = -\frac{8}{π^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{w}{w^2 + h^2}
$$

### **SCYLLA-IoU(SIoU)**

SCYLLA-IoU (SIoU) considers **Angle cost**, **Distance cost**, **Shape cost** and the penalty term is as follows.

$$
\mathcal{R}_{SIoU} = \frac{\Delta + \Omega}{2}
$$

**Angle cost**

Angle cost is calculated as follows

$$
\begin{aligned}\Lambda &= 1 - 2 \cdot \sin^2\left(\arcsin(x) - \frac{\pi}{4} \right) \\   &= 1 - 2 \cdot \sin^2\left(\arcsin(\sin(\alpha)) - \frac{\pi}{4} \right) \\&= 1 - 2 \cdot \sin^2\left(\alpha - \frac{\pi}{4} \right) \\&= \cos^2\left(\alpha - \frac{\pi}{4}\right) - \sin^2\left(\alpha - \frac{\pi}{4}\right) \\ &= \cos\left(2\alpha - \frac{\pi}{2}\right) \\ &= \sin(2\alpha) \\ \end{aligned}
$$

$$
\begin{aligned} &where \\   &x = \frac{c_h}{\sigma} = \sin(\alpha) \\  &\sigma = \sqrt{(b_{c_x}^{gt} - b_{c_x})^2 + (b_{c_y}^{gt} - b_{c_y})^2} \\  &c_h = \max(b_{c_y}^{gt}, b_{c_y}) - \min(b_{c_y}^{gt}, b_{c_y})\end{aligned}
$$

If  $\alpha > \frac{\pi}{4}$ , then $\beta = \frac{\pi}{2} - \alpha$, which is calculated as beta.

**Distance cost**

Distance cost includes Angle cost, which is calculated as follows

$$
\begin{aligned}&\Delta = \sum_{t=x,y} (1 - e^{-\gamma \rho_t}) \\  &where \\  &\rho_ x = \left(\frac{b_{c_x}^{gt} - b_{c_x}}{c_w} \right)^2, \ \rho_ y = \left(\frac{b_{c_y}^{gt} - b_{c_y}}{c_h} \right)^2, \ \gamma = 2 - \Lambda\end{aligned}
$$

Here, $c_w, c_h$ are the width and height of the smallest box containing $B$ and $B^{gt}$, unlike the Angle cost.

If we look at the Distance cost, we can see that it gets sharply smaller as $\alpha \to 0$ and larger as $\alpha \to \frac{\pi}{4}$, so $\gamma$ is there to adjust it.

**Shape cost**

Shape cost is calculated as follows

$$
\begin{aligned}&\Omega = \sum_{t=w,h} (1-e^{-\omega_t})^{\theta} \\ &\\ &where \\ &\\  &\omega_w = \frac{|w-w^{gt}|}{\max(w,w^{gt})}, \omega_h = \frac{|h-h^{gt}|}{\max(h,h^{gt})} \\   \end{aligned}
$$

The $\theta$ specifies how much weight to give to the Shape cost, usually set to 4 and can be a value between 2 and 6.

The final loss is

$$
L_{SIoU} = 1 - IoU + \frac{\Delta + \Omega}{2}
$$

## Cosine Annealing

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos{\left(\frac{T_{cur}}{T_{\max}}\pi\right)} \right), \ T_{cur} \neq (2k+1)T_{\max}
$$

$\eta_{\min}$ : min learning rate

$\eta_{\max}$ : max learning rate

$T_{\max}$ : period

# YOLOv3 Implementation

YOLOv1부터 YOLOv3까지 논문을 읽으며 각 버전이 어떤 부분을 개선했고, 어떤 아이디어를 핵심으로 두고 있는지 등 이해가 되는 부분도 많았지만, 이해가 안되는 부분도 많았다. 특히 직접 구현을 하려고 보니, 논문을 읽을 땐 알아차리지 못하고 넘어간 세세한 동작 과정들 예를 들면 predicted box와 ground truth를 어떻게 매칭 시키는지 등에 대한 이해가 부족함을 깨달았다. 

이때 발견한 것이 아래 블로그인데, YOLO 논문만 보고는 쉽게 이해할 수 없는 다양한 내용들을 설명해주어 큰 도움이 되었다. 

[One-stage object detection](https://machinethink.net/blog/object-detection/)

이렇게 YOLOv3에 대한 이해를 높이고, 코드로 구현하는 건 Aladdin Persson의 유튜브 영상을 보고 그대로 따라했다. 

[https://youtu.be/Grir6TZbc1M?si=-HOZelg3ZrjNPxP1](https://youtu.be/Grir6TZbc1M?si=-HOZelg3ZrjNPxP1)

 

영상 제목 그대로 YOLOv3의 모든 걸 처음부터 끝까지 직접 구현하기 때문에 영상을 따라하며 코드를 직접 작성하고 그 코드를 이해하는 과정에서 YOLOv3의 동작 과정에 대한 세세한 원리들을 알 수 있었다. 

Aladdin Persson의 다른 유튜브 영상 중에는 mAP(Mean Average Precision), NMS(non-maximum suppression)을 구현하는 영상도 있어서 해당 영상들도 참고하여 YOLOv3 모델을 학습 시키고 평가할 수 있도록 해보았다. 

YOLOv3 모델이 잘 돌아가는지 확인한 수 모델 코드만 다시 yolov3 architecture 그림들을 참고해 가면서 아래와 같이 구현해 보았다. 

```python
# YOLOv3 model

import torch
import torch.nn as nn

class CNNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bn_act=True, **kwargs):
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels=in_channels, out_channels=out_channels, 
            kernel_size=kernel_size, stride=stride, padding=padding, 
            bias=not bn_act, **kwargs
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.leaky = nn.LeakyReLU(0.1)
        self.use_bn_act = bn_act

    def forward(self, x):
        if self.use_bn_act:
            return self.leaky(self.bn(self.conv(x)))
        else:
            return self.conv(x)
        

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, use_residual=True, repeats=1):
        super().__init__()
        
        res_layers = []
        
        for _ in range(repeats):
            res_layers += [
                nn.Sequential(
                    CNNBlock(in_channels, in_channels//2, kernel_size=1, stride=1, padding=0, bn_act=True), # 1x1 
                    CNNBlock(in_channels//2, in_channels, kernel_size=3, stride=1, padding=1, bn_act=True) # 3x3
                )
            ]
        self.layers = nn.ModuleList(res_layers)
        self.repeats = repeats
        self.use_residual = use_residual
        
    def forward(self, x):
        for layer in self.layers:
            res = x
            x = layer(x)
            if self.use_residual:
                x = x + res  # skip connection
        return x
        

class ScalePrediction(nn.Module):
    def __init__(self, in_channels, num_classes):
        super().__init__()
        
        self.pred = nn.Sequential(
            CNNBlock(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1, bn_act=True),
            # (4 + 1 + num_classes) * 3 : 4 for [x, y, w, h], 1 for objectness prediction, 3 for anchor boxes per grid ce
            CNNBlock(in_channels*2, 3*(4+1+num_classes), kernel_size=1, stride=1, padding=0, bn_act=False)
        )
        self.num_classes =  num_classes
    
    def forward(self, x):
        output = self.pred(x)
        
        # x = [batch_num, 3*(num_classes + 5), N, N]
        output = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))
        output = output.permute(0, 1, 3, 4, 2) # output = [B x 3 x N x N x 5+num_classes]
        return output 
        

class YOLOv3(nn.Module):
    def __init__(self, in_channels=3, num_classes=20):
        super().__init__()
        self.num_classes = num_classes
        self.in_channels = in_channels  
        
        self.layers = nn.ModuleList([
            CNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1, bn_act=True), # padding=1 if kernel_size == 3 else 0
            CNNBlock(32, 64, kernel_size=3, stride=2, padding=1, bn_act=True),
            ResidualBlock(64, repeats=1),
            CNNBlock(64, 128, kernel_size=3, stride=2, padding=1, bn_act=True),
            ResidualBlock(128, repeats=2),
            CNNBlock(128, 256, kernel_size=3, stride=2, padding=1, bn_act=True),
            ResidualBlock(256, repeats=8),
            CNNBlock(256, 512, kernel_size=3, stride=2, padding=1, bn_act=True),
            ResidualBlock(512, repeats=8),
            CNNBlock(512, 1024, kernel_size=3, stride=2, padding=1, bn_act=True),
            ResidualBlock(1024, repeats=4), # To this point is Darknet-53
            CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0, bn_act=True),
            CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1, bn_act=True),
            ResidualBlock(1024, use_residual=False, repeats=1),
            CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0, bn_act=True),
            ScalePrediction(512, num_classes=num_classes),
            CNNBlock(512, 256, kernel_size=1, stride=1, padding=0, bn_act=True), # for concatenate
            nn.Upsample(scale_factor=2),
            CNNBlock(768, 256, kernel_size=1, stride=1, padding=0, bn_act=True),
            CNNBlock(256, 512, kernel_size=3, stride=1, padding=1, bn_act=True),
            ResidualBlock(512, use_residual=False, repeats=1),
            CNNBlock(512, 256, kernel_size=1, stride=1, padding=0, bn_act=True),
            ScalePrediction(256, num_classes=num_classes),
            CNNBlock(256, 128, kernel_size=1, stride=1, padding=0, bn_act=True), # for concatenate
            nn.Upsample(scale_factor=2),
            CNNBlock(384, 128, kernel_size=1, stride=1, padding=0, bn_act=True),
            CNNBlock(128, 256, kernel_size=3, stride=1, padding=1, bn_act=True),
            ResidualBlock(256, use_residual=False, repeats=1),
            CNNBlock(256, 128, kernel_size=1, stride=1, padding=0, bn_act=True),
            ScalePrediction(128, num_classes=num_classes)
        ])
        
    def forward(self, x):
        outputs = [] # for each scale
        route_connections = [] # for concatenate
        
        for layer in self.layers:
            if isinstance(layer, ScalePrediction):
                outputs.append(layer(x))
                continue # Since this is the output of each scale, it must skip x = ScalePrediction(x).
            x = layer(x)
            
            if isinstance(layer, ResidualBlock) and layer.repeats == 8:
                route_connections.append(x)
            
            elif isinstance(layer, nn.Upsample):
                x = torch.cat([x, route_connections[-1]], dim=1)
                route_connections.pop()
            
        return outputs
```

다행이 잘 동작하였고 100epoch으로 mAP@50을 측정해본 결과 31.7이라는 결과가 나왔다. 

# YOLOv4 Implementation

YOLOv3을 동영상을 따라 구현해보고 나니 


# 시행착오

LEARNING_RATE = 0.001로 했을 때 mAP 0 -> 3 -> 7 -> 9 -> 7 .. 로 큰 성능이 매우 낮았다.

loss도 처음엔 빠르게 감소하지만 나중으로 갈수록 감소속도가 매우 느려지며 학습이 거의 진행되지 않았다. (결굴 mAP 9를 최대로 오히려 성능 감소)

LEARNING_RATE = 0.00001로 했을 땐 loss 감소가 너무 느리며 학습이 거의 진행되지 않았다.

CSPResBlock구현 시 split하는 과정에서 똑같은 conv를 적용해주니까 그냥 하나의 conv를 정의하고 그 하나의 conv로 split을 해주면 된다고 생각했는데, 다시 생각해보니 각각의 conv로 따로 정의해줘야 각각의 conv 가중치가 업데이트 되면서 CSP의 역할을 할 수 있는거 같다.

확실히 CIoU를 쓰면 mAP가 빠르게 성장하는 듯?
