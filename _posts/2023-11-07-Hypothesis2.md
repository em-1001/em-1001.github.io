---
title:  "[Statistics] Hypothesis Testing II"
excerpt: "statistical hypothesis test"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> 데이터 사이언티스트 되기 책: https://recipesds.tistory.com/

## Normalization vs Standardization vs Regularization

정규화(Normalization), 정규화(Standardization), 정규화(Regularization)의 차이를 알아보자. 우선 정규화(Normalization)와 정규화(표준화, Standardization)의 차이를 보면 정규화는 영어로 Normalization, 표준화는 영어로 Standardization이다. 정규화는 모든 값을 0~1사이의 값으로 만드는 것으로 Min-Max Scaling이 여기에 속한다. 정규화를 하지 않으면 데이터 Feature마다 크기가 다 달라서 회귀분석 등을 하게 되면 더 큰 값을 가진 데이터가 더 큰 기여를 하는 것처럼 보일 수 있다. 

표준화는 표준정규분포의 속성을 갖도록 데이터를 재조정하는 것을 말한다. 이때의 조건은 $\mu=0, \sigma=1$이다. 일반적인 표준화 방법으로는 $z = \frac{x - \mu}{\sigma}$ 로써 Z-score를 만들어 변환한다. 그래서 정규화는 0~1 사이의 값을 갖지만, 표준화는 값이 제한되지는 않는다.

통계학에서는 표준화 정규화 두 가지를 구분 없이 normalization으로 부른다. 이유는 정규화란 데이터의 범위를 일치시키거나 분포를 유사하게 만들어 주는 등의 작업을 일컫는 말이기 때문이다. 그래서 데이터를 norm이나 standard로 변환하는 것을 normalize로 퉁쳐서 부르는 경향이 있다. 
이후에 나올 L1, L2 normalization을 보통 Regulariztion(일반화)라고 부른다. 

## Normality Test

지금까지는 표본의 크기가 어느정도 크다고 가정하여 중심극한정리 등을 사용해 검정을 하였다. 하지만 표본의 크기가 작다면($n \leq 30$) 방법이 달라진다. 

1. 일반적으로 모집단이 정규분포라면 표본의 평균의 분포는 정규분포를 이룬다. 즉, 표본의 크기 n이 크던 작던 관계없이 그때의 표본의 평균은 Gaussian의 outcome이다.
2. 만약 표본의 크기가 충분히 큰 경우, 중심극한정리에 의해 표본의 평균은 모집단의 분포와 관계없이 Gaussian에서의 Outcome이 된다.
3. 정규분포로 분포하는 표본평균의 분포를 구할 때에는, 모분산이 필요한데, 모분산을 모르므로 모분산 대신에 표본분산을 이용하게 되면 표본평균의 분포는 정규분포가 아니라 자유도에 따른 t 분포를 따른다.

만약 여기서 $n < 30$ 이라면 1번을 만족하는 경우 t분포로 대용할 수 있다. 하지만 1번도 만족하지 않는 소포뵨의 경우 어떻게든 정규분포한다는 조건을 만족시켜서 검정이 유효하다는 것을 주장하기 위해 그렇게도 모집단의 정규성을 검정한다. 여기서 정규성 검정(Normality Test)이 쓰인다. 만약 여기서 정규성 검정을 통해 모집단이 정규성이 있는 것으로 나오면 t검정을 사용할 수 있지만, 그렇지 않은 경우에는 비모수 검정을 해야 한다. 

정규성 검정도 검정이기 때문에 Null Hypothesis와 Alternative Hypothesis가 있다. 표본을 통해 모집단이 정규 분포하는지 검정했더니 정규분포하더라라는 결론이 나온다면, 현재 가지고 있는 표본의 평균은 정규분포에서의 outcome이라 할 수 있다. 따라서 귀무가설은 "표본의 모집단이 정규분포를 이루고 있다."이고, 대립가설은 "표본의 모집단이 정규분포를 따르지 않는다."이다. 이에 대한 검정 방법은 다음과 같은 것들이 있다. 

1. Shapiro-Wilk 검정 : 소표본에 평균을 활용한 검정을 위한 모집단 정규성 검정에 적합 
2. Kolmogorov-Smirnov 검정 : 소표본에 평균을 활용한 검정을 위한 모집단 정규성 검정에 적합
3. QQ plot : 매우 큰 표본 또는 데이터 변환시 정규에 가까워지는 변화가 있는지 확인하는 것에 많이 사용
4. 왜도와 첨도 : 매우 큰 표본을 확인하는데 적합

이 중 Shapiro-Wilk 검정이 소표본에서 가장 많이 쓰이는 정규성 검정 방법이다. Shapiro-wilk의 검정통계량의 원리는 다음과 같다. 

$$W = \frac{\left(\sum_{i=1}^n a_ix_{(i)} \right)^2}{\sum_{i=1}^n(x_i - \bar{x})^2}$$

여기서 a 계수는 Covariance Matrix로부터 계산한 값으로 그냥 검정통계량인 $W$가 이 값에 대한 확률표를 가지고 p value를 구한다 정도만 알면 된다. 

모분포를 모르는 매우 큰 대표본에서는  Shapiro-Wilk와 Kolomogorov-Smirnov 정규성 검정이 Null Hypothesis를 기각하게 될 가능성이 크다. 이유는 완벽한 정규분포로부터 조금이라도 빗나가면 아주 작은 p value를 얻기 때문이다. 
그래서 이럴때 많이 사용하는 방법이 QQ plot, 첨도/왜도이다.  QQ plot은 시각적으로 데이터의 분포를 보는 방법으로 정규성을 보이지 않는 데이터를 정규성이 보이도록 데이터 변환을 했을 때, 나아졌는지 확인할 때 유용한 방법이다. 사실 큰 대표본에서는 정규성 검정이 큰 의미가 없다. 우리가 관심 있는 것은 표본의 평균의 분포이지 모분포가 아니기 때문에 중심극한정리를 이용해서 표본평균의 분포를 정규성을 띈다고 가정할 수 있기 때문이다. 

이런 큰 표본에서 QQ plot이나, 첨도왜도로 모집단이 정규하는가를 따지는 이유는 평균을 활용한 검정을 하기 위해서라기보다는, 여러 가지로 모집단을 이해하기 훨씬 쉬워진다는 의미에서 사용한다. 특히 데이터 변환할 때, 예를 들어, 오른쪽으로 꼬리가 긴 분포는 로그 변환을 사용해주면, 표본 크기가 작더라도 정규분포와 유사한 모양이 되는 것을 QQ plot을 이용해서 확인해 볼 수 있다. 

첨도/왜도에 대한 기준은 다음과 같다. 

<p align="center"><img src="https://github.com/user-attachments/assets/1a1f9342-1eea-4bb4-b085-d10a986b910e" height="70%" width="70%"></p>

추가적으로 첨도=0 이면 표준정규분포이고, 첨도가 크더라도 분산이 더 큰 경우 완만한 그래프가 될 수 있으므로, 비교를 할 경우 분산이 동일한 분포끼리 비교해야 한다. 가우시안과 t분포가 그러한 관계이다. West 등(1995)의 연구에서는 왜도는 절대값이 2, 첨도는 절댓값이 7 이하이면 정규분포에서 크게 벗어나지 않아 정규성을 띈다고 봐도 된다고 한다. 

```py
from scipy.stats import skew, kurtosis
 
skew(data) # 왜도
kurtosis(data, fisher=True) # 첨도
```

참고로 fisher=True 이면, 정규분포 첨도를 0 기준으로 계산해주고, False이면 정규분포 첨도를 3으로 계산해 준다. 

추가적으로 정규성 검정을 하고 나면 등분산 가정이라는 것도 따라다닌다. 등분산 가정은 검정에서 비교하는 집단이 서로 분산이 같다는 가정인데, 분산이 같다는 의미는 각각의 분산이 확률변수로써 같을 확률이 크고 그렇다는 의미는 각각의 집단은 같은 성질의 집단이고, 같은 성질의 집단이라는 이야기는 같은 성질의 모집단에서 나눈 그룹일 수 있다는 의미가 된다. 따라서 각 표본의 분산의 값이 똑같을 필요는 없고, 모분산이 확률적으로 같다는 정도이다. 이런걸 동질성(Homogeneity of Variance)이라 한다. 


## $t$, $\chi^2$, $F$ testing

검정은 특정 통계량이 어떤 확률분포를 따를 때, 설정한 가설에 대해서 p value가 어떻게 되는지를 보는 것이다. 이때 어떤 확률분포에 $t$ 분포, $\chi^2$ 분포, $F$ 분포가 있다. 

앞서 살펴보았던 $t$ 분포부터 살펴보면 $t$ 분포는 표본평균의 분포가 따르는 분포이고, 모분산이 아닌 표본분산이 분포의 파라미터이다. 표본수가 작을 때는 표본정규분포보다 양쪽 꼬리가 더 두껍고, 표본크기가 커질 수록 정규분포에 가까워진다. 

$\chi^2$ 분포는 이전에 살펴보았듯이 가우시안의 제곱의 합이 해당 분포를 따른다. 카이제곱 분포는 표본분산의 분포와 관련있고, 표본분산의 검정을 할 때 사용된다. 통계량은 다음과 같다. 

$$\chi^2 = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}   
\begin{cases}
O : Observations \\  
E : Expections 
\end{cases}$$

위 통계량은 비율에 대한 검정이다. 비율은 Binomial로 표현되고, Binomial은 가우시안으로 근사가 되니까, 비율은 가우시안으로 근사된다. 
연속형 정규분포 변수에서의 카이제곱은 가우시안 제곱의 합 $\sum_{i=1}^n Z_i^2 = \sum_{i=1}^n \lbrack \frac{X_i - \mu}{\sigma} \rbrack^2 = \chi^2$ 임을 이용해서 2개 Binomial Case로 카이제곱 분포를 따르는지 유도해보면 다음과 같다. 

Binomial의 Gaussian근사를 상정하고, $\mu = np, \sigma^2 = npq$를 이용해 관측치 $O$를 z score로 쓰면,

$$z = frac{O_0 - np_0}{\sqrt{np_0(1-p_0)}} \sim N(0, 1), z^2 = \frac{(O_0 - np_0)^2}{np_0(1-p_0)} \sim \chi_{(1)}^2$$

가 된다. 이제 Binomial case 이므로 $p_0 + p_1 = 1, E_0 + E_1 = n, O_0 + O_1 = n, np_0 = E_0, np_1 = E_1$를 이용해서 유도한다. 

$$\begin{align}
Z^2 &= \frac{(O_0 - np_0)^2}{np_0(1-p_0)} = \frac{(O_0 - E_0)^2}{np_0p_1} = \frac{n(O_0 - E_0)^2}{np_0np_1} = \frac{n(O_0 - E_0)^2}{E_0E_1} \\ 
&= \frac{n(O_0 - E_0)^2}{E_0(n - E_0)} = (O_0 - E_0)^2 \left(\frac{1}{E_0} + \frac{1}{n-E_0} \right) \\ 
&= \frac{(O_0 - E_0)^2}{E_0} + \frac{((n-O_0) - (n-E_0))^2}{n - E_0} = \frac{(O_0 - E_0)^2}{E_0} + \frac{(O_1 - E_1)^2}{n-E_0} \\ 
&= \frac{(O_0 - E_0)^2}{E_0} + \frac{(O_1 - E_1)^2}{E_1} = \chi^2
\end{align}$$

이처럼 카이제곱 분포를 따르는 것을 확인할 수 있고, n개 Binomial Case로 일반화된 $\sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i} \sim \chi_{(n-1)}^2$는 $\sum Z^2$의 형태라는 것을 알 수 있다. 

마지막으로 $F$ 분포는 $\frac{\chi^2}{\chi^2}$ 형태의 확률변수가 따르는 분포로, 이때는 Continuous 형태의 데이터에서 분산과 분산의 비가 따르는 분포이다. $F$ 비는 앞으로 나올 ANOVA 분산분석에서 사용되며, $F$ 비에 사용되는 비율을 $F = \frac{\alpha}{\beta}$ 라 하면 알파 베타는 각각 설명가능한 변량의 평균, 설명하지 못하는 변량의 평균이라 한다. 이를 다르게 표현하면 다음과 같다. 

$\alpha$ : 실험을 위해 인위적으로 선택한 데이터의 분산   
$\beta$ : 표본에 의한 분산 (통제할 수 없음)

실험을 위해 인위적으로 선택했다는 것은 모델을 만든다던가, 그룹을 나눈단던가 하는 것을 말한다.

$\alpha$ : 우리가 만든 모델에 의해 예측 가능한 분산     
$\beta$ : 우리가 예측 불가능한 데이터에 의한 분산  

우리가 모델을 만들었기 때문에 원래의 상태와 모델과의 차이를 측정할 필요가 있고, 우리가 모델을 만들었기 때문에, 만든 모델과 데이터의 차이를 측정할 필요가 있게 된다. 

$\alpha$ : 우리가 뭔가를 한 것에 대한 분산        
$\beta$ : 우리가 뭔가를 한 것 이외의 분산

뭔가를 하고 난 후에 결과라는 것은 효과를 기대한 것이니, 원래 상태와 효과와의 차이를 말하는 것이다. 

$\alpha$ : 결과에 대한 효과의 분산       
$\beta$ : 결과에 대한 오차의 분산  

쉽게 축약해서 쓰면 다음과 같다. 

$\alpha$ : 효과의 분산      
$\beta$ : 오차의 분산 

전반적으로 분자는 뭔가를 함으로써 나오는 원래의 상태로부터의 차이, 분모는 뭔를 했지만 여전히 있는 차이를 말한다. 

집단 차이 분석의 ANOVA F 검정의 $F$ 비의 경우 다음과 같다. 

$\alpha$ : 집단을 나눴기 때문에 생기는 집단끼리의 차이(분산)의 평균        
$\beta$ : 집단을 나눴지만 각 집단안에 있는 차이(분산)의 평균 

회귀의 경우 $F$ 분석은 다음과 같다. 

$\alpha$ : 회귀선을 찾아냈으니까, 회귀선과 평균선과의 차이(분산)의 평균     
$\beta$ : 회귀선을 찾아냈지만, 여전히 있는 관측치와 회귀선과의 차이(분산)의 평균    

전반벅으로 (결과에 대한 변화의 차이 / 결과와 관측치의 차이)가 된다. 노이즈에 비해 얼머나 효과 차이가 나는지를 보여준다. 혹은, 모집단을 추정할 때 틀릴 수도 있는 오차에 비해, 얼마나 확실하게 차이가 큰지를 계산한다. 

- $t$검정은 표본의 평균을 비교할 때 사용할 수 있다.   

- $\chi^2$ 검정은 원래 어떤 비율이어야 하는 기대 비율에 비해 퍼진 정도를 이용해 비교하는데 사용된다. 다시말해, 기댓값으로부터 관찰값까지의 차이(거리)를 나타내는 값이다.   

- $F$ 검정은 통제 가능한 분산과 통제 불가능한 분산의 비를 이용해 통제 불가능한 것에 비해 통제 가능한 것이 차이가 나는가를 확인할 때 사용한다. 








