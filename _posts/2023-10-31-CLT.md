---
title:  "[Statistics] Central Limit Theorem & Degrees of Freedom"
excerpt: "Central Limit Theorem & Degrees of Freedom"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> 데이터 사이언티스트 되기 책: https://recipesds.tistory.com/

# Central Limit Theorem

중심극한정리(Central Limit Theorem)는 샘플링을 해서 그 **샘플에 대한 평균**을 (무한히) 계속 내 보면 그 **평균의 분포**가 실제 분포와 상관없이 가우시안 분포가 된다는 것이다. 다시말해, 적당히 큰 표본의 크기 $n$에 대해 $\bar{X}$는 평균이 $\mu$이고, 분산이 $\frac{\sigma^2}{n}$인 정규분포, 즉 $N(\mu, \frac{\sigma^2}{n})$를 따른다고 할 수 있다. 

<p align="center"><img src="https://github.com/user-attachments/assets/9e2330c8-7549-41c7-85c3-d4426338f19c"></p>

따라서 평균의 분포가 가우시안이 되기 때문에, 어떤 분포라도 평균의 분포는 예측하기 쉬워진다. 

실제 중심극한정리에 의해 평균의 분포가 정규분포에 가까워 지는 지 확인하는 방법으로는, 모집단(분석의 대상이 되는 집단)에서 $n$개식의 샘플을 $N$회 복원추출한다고 할 때, $n$이 약 30 이상의 너무 적지 않은 개수이고 시행횟수 $N$이 많아질수록 매 회 $n$개씩 뽑아 만든 평균들을 모아보면, 정규분포를 따르는 것을 확인할 수 있다. 

예를 들어 표본평균분포는 수집한 표본을 의미하는 것이 아니라, 모집단에서 표본크기가 $n$인 표본($n=30$)을 여러 번 반복해서 추출($N=200$)했을 때(즉, $X_1(n=30), X_2(n=30), X_3(n=30),...,X_200(n=30)$), 각각의 표본 평균들 ($Mean(X_1(n=30)), Mean(X_2(n=30)), Mean(X_3(n=30)),...,Mean(X_200(n=30))$)이 이루는 분포를 말한다. 그 표본의 크기가 보통 30이상으로 크면, 표본 평균들이 이루는 분포가 모집단의 평균 $\mu$, 표준편차 $\frac{\sigma}{\sqrt{n}}$인 정규분포에 가까워진다는 것이다. 

중심극한정리를 여러 버전으로 정리하면 다음과 같다. 

- 표본 평균(sample mean)들이 이루는 분포는 복원추출 횟수가 충분히 클 경우 모집단의 원래 분포와 상관없이 가우시안 분포를 따름.
- 동일한 확률 분포를 따르고 서로 독립인 $n$ 개의 확률 변수의 관찰값의 평균값은 $n$ 이 충분히 크다면 가우시안 분포를 따름.
- $n$ 개의 확률 변수가 어떤 확률 분포를 따르든지 상관없이 $n$ 이 충분히 크다면 그 합은 가우시안 분포를 따름.(Generalized CLT theorem)

주의할 점은 표본의 값을 확률변수로 하는 것이 아니라, 표본의 평균값이나, 합을 확률 변수로 설정해야 중심극한정리를 따른다는 것이다. 

또한 모집단(분석의 대상이 되는 집단)에서 $n$개씩의 샘플을 $N$회 복원 추출한다고 할 때, $n$이 약 30이상의 너무 적지 않은 개수이고, 시행 횟수 $N$이 많아질수록 매 회 $n$개를 뽑아 만든 평균들이 가우시안을 따른다는 것은 틀린 말이다. 
$N$이 아니라 $n$이 커질수록 그 표본평균의 분포가 정규분포에 가까워지게 된다. $n$을 가만히 두고 $N$만 키우면 표본 평균의 분포가 정규분포에 '점점 더 가까워지는' 게 아니라, 정규분포와는 다소간의 차이가 있는 표본 평균의 분포를 점점 더 정확히 묘사할 수 있게 되는 것뿐이다. $n$이 결정되는 순간 표본 평균의 분포도 이미 결정되는 것이고, $N$을 늘려도 그 분포가 정규분포에 더 가까워지지 않는다. 


# Degrees of Freedom
## Sample Variance

분산은 관측값과 평균의 차이를 제곱한 값들의 합을 $n$으로 나눈다. 하지만 실제 표본분산을 구할 때는 $n$이 아닌, $n-1$로 나누고 이 $n-1$을 자유도(Degrees of Freedom)라고 부른다. 이는 표본분산을 통해 모집단의 분산인 모분산을 추정하기 위함이다. 모분산보다 표본분산이 더 작게 나와서 이를 보정하기 위해 $n-1$로 나눈다는 것이다. 이렇게 하면 "확률적"으로 모분산의 추정 값이 된다고 한다. 이렇게 계산한 추정량을 불편추정량이라 한다. 

실제로 왜 $n-1$로 나누게 되는건지 증명해보자. 첫번재 증명은 모분산과 $n$으로 나눈 표본분산의 차이를 구하는 방법을 사용한다. 
$n$크기의 모집단에서 중복을 허용하여 추출한 $n$개의 표본을 이용하여 모분산과 표본분산의 차이를 계산한다. 이때 표본분산은 $n-1$이 아니라, $n$으로 나눈 표본분산이다. 

$$\begin{align}
E \lbrack \sigma^2 - s_n^2 \rbrack &= E \lbrack \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2 - \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2 \rbrack \\ 
&= E \lbrack \frac{1}{n}\sum_{i=1}^n ((x_i^2 - 2x_i\mu + \mu^2) - (x_i^2 - 2x_i\bar{x} + \bar{x}^2)) \rbrack \\ 
&= E \lbrack \frac{1}{n}\sum_{i=1}^n (\mu^2 - \bar{x}^2 + 2x_i(\bar{x} - \mu)) \rbrack \\ 
&= E \lbrack \mu^2 - \bar{x}^2 + \frac{1}{n}\sum_{i=1}^n 2x_i(\bar{x} - \mu) \rbrack \\  
&= E \lbrack \mu^2 - \bar{x}^2 + 2(\bar{x} - \mu)\bar{x} \rbrack \\ 
&= E \lbrack \mu^2 - 2\bar{x}\mu + \bar{x}^2 \rbrack \\  
&= E \lbrack (\bar{x} - \mu)^2 \rbrack \\  
&= Var(\bar{x}) \\  
&= \frac{\sigma^2}{n} \quad \because \ {\scriptstyle\text{The variance of sample mean from Central Limit Thm.
}}
\end{align}$$

위 결과를 통해 모분산과 표본분산의 차이가 $\frac{\sigma^2}{n}$만큼 나는 것을 알 수 있다. 따라서 아래가 성립한다. 

$$E \lbrack s_n^2 \rbrack = \sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2$$  

$$\sigma^2 = \frac{n}{n-1}E\lbrack s_n^2 \rbrack = \frac{n}{n-1} \cdot \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 = \frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}$$

이처럼 표본분산을 계산할 때, $n$대신 $n-1$로 나누면 모분산과 비슷해 지는것을 알 수 있다. 

자유도 설명에 앞서 모분산보다 표본분산이 더 작게 나와서 이를 보정하기 위해 $n-1$로 나눈다 했는데, 그 이유를 설명하면 모분산은 직관적으로 다음과 같다. 

표본 값들이 모평균에서 퍼진 정도(=모분산=$\sigma^2$) = 표본값들이 표본 평균에서 퍼진 정도 + 표본 평균자체가 퍼진 정도(=표본평균의 분산=$\frac{\sigma^2}{n} \to$ 중심극한정리)

따라서 표본값들이 표본 평균에서 퍼진 정도 = $\sigma^2 - \frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2$ 가 되어 표본분산이 모분산보다 작다. 


## Degrees of Freedom

자유도(Degrees of Freedom)는 주어진 조건에서 자유롭게 변화할 수 있는(Independent) 데이터의 수이다. 
$x_1, x_2, \cdots, x_n$의 observation이 있을 때 평균을 계산하여 알게되면, 원래 observation중 한 값을 잃어도 평균값을 알고 있기 때문에 정보는 전혀 손실되지 않는다. 따라서 평균을 알게된 순간 자료 중 어느 한 값은 항상 자유롭지 않은 의미 없는 정보가 되고, 자유도는 'observation-1' 즉, $n-1$이 된다. 따라서 처음 observation에 대해 평균을 계산하는 시점에는 자유도가 $n$이고, 평균으로 분산을 계산하는 시점에는 자유도가 $n-1$이 된다. 

통계는 무엇이든 평균을 내는 것이 기본이고, 흔히 구하는 평균도 평균, 분산도 평균이다. 이 평균을 구할 때 의미를 가지고 있는 데이터의 개수로 나누는 것이 통계의 기본이다. 이때 의미있는 데이터의 수가 자유도이다. 말 그대로 자유롭게 변할 수 있는 값을 가질 수 있는 변수의 수를 말한다. 통계에서는 이러한 변수의 수(자유도)를 의미있는 데이터의 수라 한다. 통계에서는 자유도만큼의 데이터만 사용해서 통계 값을 구해야 Unbiased Estimation이 되고, 자유도를 고려하지 않고 $n$개의 데이터를 그대로 통계값을 구하는데 사용하면 Biased Estimation이 된다.

예를 들어, 표본평균을 구할 때는 $n$개의 데이터가 모두 있어야 표본평균을 구할 수 있다. 따라서 이 때의 자유도는 $n$이고, 평균을 구할 때 $n$으로 나눈다. 표본분산을 구할 때는 $n$개의 데이터와 $\hat{\mu}$가 필요한데, $\hat{\mu}$는 이미 구했고, $\hat{\mu}$가 정해지니까 $n-1$개의 데이만 있으면 나머지 1개의 데이터는 자연스럽게 알 수 있는 값이 된다. 따라서 이 때의 자유도는 $n-1$이 되어 분산을 구할 때 자유도인 $n-1$로 나누게 된다. 

일반적으로 생각하면, $n$개의 observation에서 parameter(모수)를 구할 때마다 한 개의 observation이 통계적인 의미를 잃어게 된다. 

다른 예로 Linear Regression (단순 회귀)에서 표본분산의 자유도를 따져본다면, $y=ax+b$를 Regression한다고 했을 때, 최종 자유도는 모수인 기울기와 $y$절편 즉, $a, b$ 두 개의 값을 Obervation으로 구한 다음에야 $e_i$를 계산할 수 있으니까, $e_i$를 구할 때의 최종 자유도는 $n-2$가 된다. 

$$s^2 = \frac{\sum_{i=1}^n e_i^2}{n-2} = \frac{SSE}{n-2}$$

앞서 Unbiased Estimation에서 Unbiased (불편-편향되지 않다)는 치우침이 없다는 뜻이다. 여기서의 치우침은 '확률을 고려한 치우침'이다.
이는 어떤 추정량의 수학적 기댓값이 모수가 된다는 것이다. 모집단에서 추출한 $n$개의 랜덤샘플이 $x_1, x_2, \cdots, x_n$이고, 우리가 관심있는 모수가 $\theta$라 할 때, 우리는 추출된 표본들을 이용해 모수의 추정량 $\hat{\theta}$를 만든다. 여기서 편의(bias)는 $E[\hat{\theta}] - \theta$로 나타낼 수 있고, 불편추정량은 편의가 없는 추정량이니 $E[\hat{\theta}] - \theta = 0$ 즉, $E[\hat{\theta}] = \theta$를 만족하면 $\hat{\theta}$을 $\theta$의 불편추정량이라 한다. 

아래 예시를 통해 자유도를 구하는 연습을 해보자. 

| |당뇨|정상|전체|
|-|-|-|-|
|근시|a|b|30|
|정상|c|d|70|
|전체|20|80|100|

위 표의 자유도를 구해보면 만약 a를 알게 되었다고 할 때, 나머지 b, c, d를 다 구할 수 있게 된다. 같은 방식으로 b, c, d에 대해서도 마찬가지 이다. 따라서 위 표의 자유도는 1이다. 하나의 값이 정해지면 나머지 값들은 자연스럽게 정해진다. 

| |당뇨|무좀|정상|전체|
|-|-|-|-|-|
|근시|a|b|c|30|
|정상|d|e|f|70|
|전체|25|35|40|100|

위 표의 자유도는 a, b를 구하면 나머지 모든 값을 알 수 있다. 따라서 자유도는 6-4로 2이다. 
