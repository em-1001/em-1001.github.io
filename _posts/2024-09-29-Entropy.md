---
title:  "Entropy, Cross Entropy & KL divergence"
excerpt: "Information Theory"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> [Skywalk님의 블로그](https://hyunw.kim/blog/2017/10/14/Entropy.html)를 보고 정리한 내용입니다. 

# Quantity of information

정보량(Quantity of information)은 어떤 내용을 표현하기 위해 물어야 하는 최소한의 질문 개수로 생각할 수 있다. 예를 들어 알파벳 한 글자를 0과 1로 전송해야 한다고 하면 어떻게 해야 할까? 최적의 정보량으로 보내려면 그 글자가 26개의 알파벳 중 앞쪽 절반(a ~ m)에 속하는지, 뒤족 절반(n ~ z)에 속하는지 묻는거다. 이에 대한 답을 0 또는 1로 할 수 있고, 이렇게 최소 5번만 물으면 글자 하나를 알아낼 수 있다. 수학적으로 다음과 같이 표현할 수 있다. 

$$2^{질문개수} = 26$$

$$질문개수 = \log_2(26)$$

정보량을 일반화 하면 다음과 같다. 

$$\begin{align}
I &= n\log(s) \\ 
&= \log(s^n)
\end{align}$$

$n$는 보내려는 문자열의 길이, $s$는 각 선택에서 가능한 경우의 수(알파벳의 경우 26)이다. 따라서 $s^n$는 총 경우의 수가 된다. 이렇게 어떠한 사건에 대한 정보를 수치화한 것을 정보량이라 한다.  

정보량은 어떠한 사건이 일어날 확률과 반비례한다. 이는 수식으로 아래와 같이 표현된다. 

$$\begin{align}
I(X) &= \log(\frac{1}{p(x)}) \\  
&= -\log(p(x))   
\end{align}$$

정보량에 로그함수가 사용되는 이유는 첫 번째로 정보에 필요한 최소한의 자원(질문)을 표현할 수 있기 때문이다. 확률이 $\frac{1}{4}$인 사건을 2진수로 표현하면 $-\log_2(\frac{1}{4}) = 2$ bit로 표현할 수 있다. 2는 여기서 필요한 최소한의 질문 수이다.  

두 번째는 log함수의 additive한 성질 때문이다. 독립 사건 A, B에 대해 두 사건이 동시에 일어날 확률 $P(A)P(B)$의 정보량 $I(A, B)$는 $I(A) + I(B)$로 쪼개질 수 있다. 

$$\begin{align}
I(A, B) &= -\log(P(A)P(B)) \\  
&= -\log(P(A)) - \log(P(B)) \\   
&= I(A) + I(B)
\end{align}$$

# Entropy

예를 들어 문자열을 출력하는 2개의 기계 $X$와 $Y$가 있다고 하자. 기계 $X$는 a, b, c, d를 각각 0.25의 확률로 출력한다. 
반면 기계 $Y$는 a: 0.5, b: 0.125, c: 0.125, d: 0.25의 확률로 출력한다. 

기계 $X$가 출력하는 문자 1개를 구분하기 위해 필요한 최소한의 질문 수는 2이다. 확률이 모두 0.25이므로 반 씩 나누어 질문하면 되기 때문이다. 기계 $Y$의 경우 $X$의 방식은 비효율적이다. 왜냐하면 a가 이미 50%의 확률을 갖기 때문이다. 그러므로 처음에 a인지, (b,c,d) 중에 있는지 묻는게 낫다. 이후 (b,c,d)중에선 d가 0.5중에 0.25로 50%의 확률을 가지므로 d인지, (b,c) 중에 있는지 물으면 된다. 기계 $Y$의 최소 질문 수는 앞서 $X$처럼 계산되지 않느다. 대신 a가 나타날 확률에 a를 추려내기 위한 질문 수 (처음 1개의 질문으로 추려낼 수 있다.)를 곱하고, b도 b가 나타날 확률에 b를 추려내기 위한 질문의 수(3번의 질문으로 특정된다.)를 곱하며 나머지 c,d에 대해서도 똑같이 한다. 이를 식으로 나타내면 다음과 같다. 

$$p(a) \cdot 1 + p(b) \cdot 3 + p(c) \cdot 3 + p(d) \cdot 2 = 1.75$$

기계 $Y$에서 글자 1개를 추려내려면 평균적으로 1.75개의 질문이 필요하다. 결과적으로 기계 $Y$가 기계 $X$보다 더 적은 정보량을 생산한다고 볼 수 있다. 이는 기계 $Y$의 불확실성이 더 적기 때문이다. 기계 $X$와 같이 모든 사건이 같은 확률로 일어나는 것이 가장 불확실하다. 이를 식으로 정립한 것이 Claude Shannon으로 Shannon은 이 불확실성 측정을 엔트로피라 불렀으며 단위를 bit로 하였다. 

기계 $Y$의 평균 질문 수(정보량)를 계산하는 식은 각각이 발생할 확률과 관련있다.  
$p(a): 0.5=1/2, p(b): 0.125=1/8, p(c): 0.125=1/8, p(d): 0.25=1/4$ 이므로 각각의 확률에 대한 정보량을 구하면

$$\log_2 \left(\frac{1}{1/2}\right) = 1, \log_2 \left(\frac{1}{1/8}\right) = 3, \log_2 \left(\frac{1}{1/4}\right) = 2$$

가 된다. 즉 정보량에서의 가능한 경우의 수는 해당 사건이 발생할 확률의 역수가 되고 위에서 구해진 정보량에 확률을 곱한 값이 엔트로피가 된다. Shannon의 엔트로피 식은 이산확률분포일 때를 가정하므로 시그마를 통해 아래와 같이 엔트로피 식이 일반화 된다. 

$$\begin{align}
H &= \sum_i p_i \log \left( \frac{1}{p_i} \right) \\ 
&= - \sum_i p_i \log(p_i)
\end{align}$$

엔트로피는 최적의 전략 하에 특정 사건을 예측하는 데 필요한 질문 수를 의미한고, 다른 말로는 질문 수에 대한 **기댓값(Expected Value)** 이 된다. 엔트로피가 감소한다는 것은 해당 사건을 특정하기 위해 필요한 질문의 수가 줄어든다는 것을 의미하고, 질문의 수가 줄어든다는 것은 정보량이 줄어든다는 의미가 된다. 

# Cross Entropy

# KL Divergence
