---
title:  "Entropy, Cross Entropy & KL divergence"
excerpt: "Information Theory"

categories:
  - Statistics
tags:
  - Statistics
toc: true
toc_sticky: true
last_modified_at: 2025-01-19T08:06:00-05:00
---

> [Skywalk님의 블로그](https://hyunw.kim/blog/2017/10/14/Entropy.html)를 보고 정리한 내용입니다. 

# Quantity of information

정보량(Quantity of information)은 어떤 내용을 표현하기 위해 물어야 하는 최소한의 질문 개수로 생각할 수 있다. 예를 들어 알파벳 한 글자를 0과 1로 전송해야 한다고 하면 어떻게 해야 할까? 최적의 정보량으로 보내려면 그 글자가 26개의 알파벳 중 앞쪽 절반(a ~ m)에 속하는지, 뒤족 절반(n ~ z)에 속하는지 묻는거다. 이에 대한 답을 0 또는 1로 할 수 있고, 이렇게 최소 5번만 물으면 글자 하나를 알아낼 수 있다. 수학적으로 다음과 같이 표현할 수 있다. 

$$2^{질문개수} = 26$$

$$질문개수 = \log_2(26)$$

이 개념을 정립한 사람이 R.V.L Hartley이고, 그는 논문에 정보를 $H$라 표현한다. 

$$\begin{align}
H &= n\log(s) \\ 
&= \log(s^n)
\end{align}$$

$n$는 보내려는 문자열의 길이, $s$는 각 선택에서 가능한 경우의 수(알파벳의 경우 26)이다. 이러한 정보량을 수치화한 것이 정보량이다. 

정보량은 어떠한 사건이 일어날 확률과 반비례한다. 이는 수식으로 아래와 같이 표현된다. 

$$\begin{align}
I(X) &= \log(\frac{1}{p(x)}) \\  
&= -\log(p(x))   
\end{align}$$

정보량에 로그함수가 사용되는 이유는 첫 번째로 정보에 필요한 최소한의 자원(질문)을 표현할 수 있기 때문이다. 확률이 $\frac{1}{4}$인 사건을 2진수로 표현하면 $-\log_2(\frac{1}{4}) = 2$ bit로 표현할 수 있다. 2는 여기서 필요한 최소한의 질문 수이다.  

두 번째는 log함수의 additive한 성질 때문이다. 독립 사건 A, B에 대해 두 사건이 동시에 일어날 확률 $P(A)P(B)$의 정보량 $I(A, B)$는 $I(A) + I(B)$로 쪼개질 수 있다. 

$$\begin{align}
I(A, B) &= -\log(P(A)P(B)) \\  
&= -\log(P(A)) - \log(P(B)) \\   
&= I(A) + I(B)
\end{align}$$

# Entropy




# Cross Entropy

# KL Divergence
