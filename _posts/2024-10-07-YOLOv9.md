---
title:  "YOLOv9 Paper"
excerpt: "YOLOv9 Paper"

categories:
  - Computer Vision
tags:
  - Object Detection
  - Paper
toc: true
toc_sticky: true
last_modified_at: 2024-10-07T08:06:00-05:00
---

# YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information

많은 input data가 feedforward process에서 상당량의 정보손실을 겪고 이러한 현상은 편향된 gradient flows를 일으킬 수 있다.
이러한 현상을 information bottleneck이라 하며 아래 사진에서 확인할 수 있다. 

![image](https://github.com/user-attachments/assets/7a55496a-bdeb-4902-8ca5-cd89aeb8a572)

본 논문은 위와 같은 현상을 해결하기 위해  **programmable gradient information(PGI)** 를 제안한다. 
이는 auxiliary reversible branch를 통해 신뢰할 만한 gradients를 생성하는 것이다. 
gradient information propagation 각기 다른 semantic levels에서 프로그래밍하여 최고의 training 결과를 내도록 한다. 
또한 이 PGI는 auxiliary branch에서 만들어 지기 때문에 추가적인 비용이 없이 자유롭게 target task에 맞는 loss function를 선택할 수 있다. 

본 논문은 또한 기존 ELAN에 기반한 **generalized ELAN(GELAN)** 을 제안한다. 기존 ELAN과의 차이점은 임의로 적절한 computational blocks를
선택할 수 있게하여 상황에 맞는 inference가 가능하게 한다. 

## Information Bottleneck Principle

$$I(X, X) \ge I(X, f_{\theta}(X)) \ge I(X, g_{\phi}(f_{\theta}(X)))$$ 

information bottleneck은 위 수식처럼 변환 과정을 거치면서 data loss가 발생한다. $I$는 mutual information, $f$, $g$는 변환 함수,
$\theta$, $\phi$는 각 함수의 파라미터이다. $f_{\theta}(\cdot)$와 $g_{\phi}(\cdot)$는 DNN에서 연이은 layer 연산을 의미한다. 
위 식에서 알 수 있듯이 네트워크가 깊어질수록 본래 데이터가 소실된다. 

이러한 문제를 해결할 방법중 하나는 직접적으로 모델의 크기를 키워서 모델의 파라미터 수를 늘리는 것이다. 하지만 이렇게 해서 target에
사용될 충분한 양의 정보를 확보할 수는 있겠지만, 근본적으로 신뢰할 수 없는 gradients문제를 해결하지는 못한다. 

이러한 문제를 해결하기 위해 reversible functions(가역함수)가 사용된다. 

$$X = v_{\zeta}(r_{\psi}(X))$$

위 처럼 함수 $r$이 역함수 $v$를 가질 때, 이 함수를 reversible function이라 한다. $\psi$와 $\zeta$는 각 함수의 파라미터이다. 

$$I(X, X) = I(X, r_{\psi}(X)) = I(X, v_{\zeta}(r_{\psi}(X)))$$ 

reversible function을 사용해서 위 식처럼 정보 손실 없이도 네트워크를 통과할 수 있다. 
따라서 reversible function이 network transformation에 쓰인다면, 더 신뢰할 수 있는 gradients가 모델을 업데이트하는데 사용될 수 있다. 

오늘날 대부분의 유명한 deep learning method들은 아래와 같이 reversible property를 따르는 아키텍쳐를 사용한다. 

$$X^{l+1} = X^l + f_{\theta}^{l+1}(X^l)$$

$l$은 PreAct ResNet의 $l$-th layer를 뜻하고 $f$는 $l$-th layer의 transformation function을 뜻한다. 
이러한 설계는 수많은 layer를 가진 deep neural networks의 경우 잘 수렴할 수 있도록 해주지만, 이는 우리가 deep neural networks를 사용하는 이유가 아니다. 즉, 어려운 문제에 대해 data와 target사이의 간단한 매핑 함수를 찾는 것은 어렵다. 이 떄문에 layer의 수가 적을 때 PreAct ResNet는 ResNet보다 저조한 성능을 보인다. 

논문은 아래와 같이 masked modeling 사용을 시도했다. 

$$X = v_{\zeta}(r_{\psi}(X) \cdot M)$$

이는 $r$함수의 inverse transformation인 $v$를 찾기 위한 approximation methods이다. $M$은 동적 이진 mask이다.
위와 같은 방법은 diffusion model이나 variational autoencoder(VAE)에서도 사용되며, 모두 inverse function을 찾는것이 목적이다. 
그러나 위와 같은 접근을 경량화 모델에 사용하면 많은 걍의 raw data에 대한 파라미터 부족으로 문제가 발생한다. 
이러한 이유로 data $X$를 target $Y$에 매핑하는 중요 정보 $I(Y, X)$역시 같은 문제를 겪을 것이다.

이러한 문제를 information bottleneck을 이용해서 살펴보면 아래와 같다. 

$$I(X, X) \ge I(Y, X) \ge I(Y, f_{\theta}(X)) \ge ... \ge I(Y, \hat{Y})$$

$I(X, Y)$는 $I(X, X)$에 비해 매우 작은 부분이지만, 모델의 목표에 매우 중요하다. 따라서 feedforward 단계에서 잃게되는 정보의 양이 중요하지 않더라도 거기에 $I(X, Y)$가 포함된다면 훈련에 큰 영향을 끼치게 된다. 경량화 모델은 그 자체로 파라미터가 적은 상태이기 때문에 feedforward 단계에서 중요 정보를 잃을 가능성이 높다. 그래서 경량화 모델을 학습시키는데 주요 목표는 $I(X, X)$로 부터 $I(X, Y)$를 적절히 필터시키는 것이다. 

## Programmable Gradient Information(PGI)

![image](https://github.com/user-attachments/assets/5942b68f-5bd3-4fa8-a2fe-81e70d07396e)

PGI는 main branch, auxiliary reversible branch, multi-level auxiliary information으로 크게 3개로 구성되어있다. 
위 그림의 (d)를 보면 inference 단계에선 오직 main branch만을 사용하기 때문에 추가적인 inference cost가 발생하지 않는다. 
나머지 두 개의 요소는 deep learning method의 중요 문제를 해결하기 위해 사용된다. 

### Auxiliary Reversible Branch







